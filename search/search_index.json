{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"PlanOpticon","text":"<p>AI-powered video analysis and knowledge extraction.</p> <p>PlanOpticon processes video recordings into structured knowledge \u2014 transcripts, diagrams, action items, key points, and knowledge graphs. It auto-discovers available models across OpenAI, Anthropic, and Gemini, and produces rich multi-format output.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Multi-provider AI \u2014 Automatically discovers and routes to the best available model across OpenAI, Anthropic, and Google Gemini</li> <li>Smart frame extraction \u2014 Change detection for transitions + periodic capture (every 30s) for slow-evolving content like document scrolling</li> <li>People frame filtering \u2014 OpenCV face detection removes webcam/video conference frames, keeping only shared content (slides, documents, screen shares)</li> <li>Diagram extraction \u2014 Vision model-based classification detects flowcharts, architecture diagrams, charts, and whiteboards</li> <li>Knowledge graphs \u2014 Extracts entities and relationships, builds and merges knowledge graphs across videos</li> <li>Action item detection \u2014 Finds commitments, tasks, and follow-ups with assignees and deadlines</li> <li>Batch processing \u2014 Process entire folders of videos with merged knowledge graphs and cross-referencing</li> <li>Rich output \u2014 Markdown, HTML, PDF, Mermaid diagrams, SVG/PNG renderings, JSON manifests</li> <li>Cloud sources \u2014 Fetch videos from Google Drive and Dropbox shared folders</li> <li>Checkpoint/resume \u2014 Pipeline resumes from where it left off if interrupted \u2014 no wasted work</li> <li>Screengrab fallback \u2014 When extraction isn't perfect, captures frames with captions \u2014 something is always better than nothing</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code># Install from PyPI\npip install planopticon\n\n# Analyze a single video\nplanopticon analyze -i meeting.mp4 -o ./output\n\n# Process a folder of videos\nplanopticon batch -i ./recordings -o ./output --title \"Weekly Meetings\"\n\n# See available AI models\nplanopticon list-models\n</code></pre>"},{"location":"#installation","title":"Installation","text":"PyPI (Recommended)With cloud sourcesWith everythingBinary (no Python needed) <pre><code>pip install planopticon\n</code></pre> <pre><code>pip install planopticon[cloud]\n</code></pre> <pre><code>pip install planopticon[all]\n</code></pre> <p>Download the latest binary for your platform from GitHub Releases.</p>"},{"location":"#requirements","title":"Requirements","text":"<ul> <li>Python 3.10+</li> <li>FFmpeg (for audio extraction)</li> <li>At least one API key: <code>OPENAI_API_KEY</code>, <code>ANTHROPIC_API_KEY</code>, or <code>GEMINI_API_KEY</code></li> </ul>"},{"location":"#license","title":"License","text":"<p>MIT License \u2014 Copyright (c) 2026 CONFLICT LLC. All rights reserved.</p>"},{"location":"cli-reference/","title":"CLI Reference","text":""},{"location":"cli-reference/#planopticon-analyze","title":"<code>planopticon analyze</code>","text":"<p>Analyze a single video and extract structured knowledge.</p> <pre><code>planopticon analyze [OPTIONS]\n</code></pre> Option Type Default Description <code>-i</code>, <code>--input</code> PATH required Input video file path <code>-o</code>, <code>--output</code> PATH required Output directory <code>--depth</code> <code>basic\\|standard\\|comprehensive</code> <code>standard</code> Processing depth <code>--focus</code> TEXT all Comma-separated focus areas <code>--use-gpu</code> FLAG off Enable GPU acceleration <code>--sampling-rate</code> FLOAT 0.5 Frame sampling rate (fps) <code>--change-threshold</code> FLOAT 0.15 Visual change threshold <code>--periodic-capture</code> FLOAT 30.0 Capture a frame every N seconds regardless of change (0 to disable) <code>--title</code> TEXT auto Report title <code>-p</code>, <code>--provider</code> <code>auto\\|openai\\|anthropic\\|gemini\\|ollama</code> <code>auto</code> API provider <code>--vision-model</code> TEXT auto Override vision model <code>--chat-model</code> TEXT auto Override chat model"},{"location":"cli-reference/#planopticon-batch","title":"<code>planopticon batch</code>","text":"<p>Process a folder of videos in batch.</p> <pre><code>planopticon batch [OPTIONS]\n</code></pre> Option Type Default Description <code>-i</code>, <code>--input-dir</code> PATH required Directory containing videos <code>-o</code>, <code>--output</code> PATH required Output directory <code>--depth</code> <code>basic\\|standard\\|comprehensive</code> <code>standard</code> Processing depth <code>--pattern</code> TEXT <code>*.mp4,*.mkv,*.avi,*.mov,*.webm</code> File glob patterns <code>--title</code> TEXT <code>Batch Processing Results</code> Batch title <code>-p</code>, <code>--provider</code> <code>auto\\|openai\\|anthropic\\|gemini\\|ollama</code> <code>auto</code> API provider <code>--vision-model</code> TEXT auto Override vision model <code>--chat-model</code> TEXT auto Override chat model <code>--source</code> <code>local\\|gdrive\\|dropbox</code> <code>local</code> Video source <code>--folder-id</code> TEXT none Google Drive folder ID <code>--folder-path</code> TEXT none Cloud folder path <code>--recursive/--no-recursive</code> FLAG recursive Recurse into subfolders"},{"location":"cli-reference/#planopticon-list-models","title":"<code>planopticon list-models</code>","text":"<p>Discover and display available models from all configured providers.</p> <pre><code>planopticon list-models\n</code></pre> <p>No options. Queries each provider's API and displays models grouped by provider with capabilities.</p>"},{"location":"cli-reference/#planopticon-clear-cache","title":"<code>planopticon clear-cache</code>","text":"<p>Clear API response cache.</p> <pre><code>planopticon clear-cache [OPTIONS]\n</code></pre> Option Type Default Description <code>--cache-dir</code> PATH <code>$CACHE_DIR</code> Path to cache directory <code>--older-than</code> INT all Clear entries older than N seconds <code>--all</code> FLAG off Clear all cache entries"},{"location":"cli-reference/#planopticon-agent-analyze","title":"<code>planopticon agent-analyze</code>","text":"<p>Agentic video analysis \u2014 adaptive, intelligent processing that adjusts depth and focus based on content.</p> <pre><code>planopticon agent-analyze [OPTIONS]\n</code></pre> Option Type Default Description <code>-i</code>, <code>--input</code> PATH required Input video file path <code>-o</code>, <code>--output</code> PATH required Output directory <code>--depth</code> <code>basic\\|standard\\|comprehensive</code> <code>standard</code> Initial processing depth (agent may adapt) <code>--title</code> TEXT auto Report title <code>-p</code>, <code>--provider</code> <code>auto\\|openai\\|anthropic\\|gemini\\|ollama</code> <code>auto</code> API provider <code>--vision-model</code> TEXT auto Override vision model <code>--chat-model</code> TEXT auto Override chat model"},{"location":"cli-reference/#planopticon-auth","title":"<code>planopticon auth</code>","text":"<p>Authenticate with a cloud storage service for batch processing.</p> <pre><code>planopticon auth SERVICE\n</code></pre> Argument Values Description <code>SERVICE</code> <code>google\\|dropbox</code> Cloud service to authenticate with <p>Examples:</p> <pre><code># Authenticate with Google Drive (interactive OAuth2)\nplanopticon auth google\n\n# Authenticate with Dropbox\nplanopticon auth dropbox\n</code></pre> <p>After authentication, use <code>planopticon batch --source gdrive</code> or <code>--source dropbox</code> to process cloud videos.</p>"},{"location":"cli-reference/#global-options","title":"Global options","text":"Option Description <code>-v</code>, <code>--verbose</code> Enable debug-level logging <code>--version</code> Show version and exit <code>--help</code> Show help and exit"},{"location":"contributing/","title":"Contributing","text":""},{"location":"contributing/#development-setup","title":"Development setup","text":"<pre><code>git clone https://github.com/ConflictHQ/PlanOpticon.git\ncd PlanOpticon\npython -m venv .venv\nsource .venv/bin/activate\npip install -e \".[dev]\"\n</code></pre>"},{"location":"contributing/#running-tests","title":"Running tests","text":"<pre><code># Run all tests\npytest tests/ -v\n\n# Run with coverage\npytest tests/ --cov=video_processor --cov-report=html\n\n# Run a specific test file\npytest tests/test_models.py -v\n</code></pre>"},{"location":"contributing/#code-style","title":"Code style","text":"<p>We use:</p> <ul> <li>Ruff for linting</li> <li>Black for formatting (100 char line length)</li> <li>isort for import sorting</li> <li>mypy for type checking</li> </ul> <pre><code>ruff check video_processor/\nblack video_processor/\nisort video_processor/\nmypy video_processor/ --ignore-missing-imports\n</code></pre>"},{"location":"contributing/#project-structure","title":"Project structure","text":"<p>See Architecture Overview for the module structure.</p>"},{"location":"contributing/#adding-a-new-provider","title":"Adding a new provider","text":"<ol> <li>Create <code>video_processor/providers/your_provider.py</code></li> <li>Extend <code>BaseProvider</code> from <code>video_processor/providers/base.py</code></li> <li>Implement <code>chat()</code>, <code>analyze_image()</code>, <code>transcribe_audio()</code>, <code>list_models()</code></li> <li>Register in <code>video_processor/providers/discovery.py</code></li> <li>Add tests in <code>tests/test_providers.py</code></li> </ol>"},{"location":"contributing/#adding-a-new-cloud-source","title":"Adding a new cloud source","text":"<ol> <li>Create <code>video_processor/sources/your_source.py</code></li> <li>Implement auth flow and file listing/downloading</li> <li>Add CLI integration in <code>video_processor/cli/commands.py</code></li> <li>Add tests and docs</li> </ol>"},{"location":"contributing/#license","title":"License","text":"<p>MIT License \u2014 Copyright (c) 2025 CONFLICT LLC. All rights reserved.</p>"},{"location":"api/analyzers/","title":"Analyzers API Reference","text":""},{"location":"api/analyzers/#video_processor.analyzers.diagram_analyzer","title":"<code>video_processor.analyzers.diagram_analyzer</code>","text":"<p>Diagram analysis using vision model classification and single-pass extraction.</p>"},{"location":"api/analyzers/#video_processor.analyzers.diagram_analyzer.DiagramAnalyzer","title":"<code>DiagramAnalyzer</code>","text":"<p>Vision model-based diagram detection and analysis.</p> Source code in <code>video_processor/analyzers/diagram_analyzer.py</code> <pre><code>class DiagramAnalyzer:\n    \"\"\"Vision model-based diagram detection and analysis.\"\"\"\n\n    def __init__(\n        self,\n        provider_manager: Optional[ProviderManager] = None,\n        confidence_threshold: float = 0.3,\n    ):\n        self.pm = provider_manager or ProviderManager()\n        self.confidence_threshold = confidence_threshold\n\n    def classify_frame(self, image_path: Union[str, Path]) -&gt; dict:\n        \"\"\"\n        Classify a single frame using vision model.\n\n        Returns dict with is_diagram, diagram_type, confidence, brief_description.\n        \"\"\"\n        image_bytes = _read_image_bytes(image_path)\n        raw = self.pm.analyze_image(image_bytes, _CLASSIFY_PROMPT, max_tokens=512)\n        result = _parse_json_response(raw)\n        if result is None:\n            return {\n                \"is_diagram\": False,\n                \"diagram_type\": \"unknown\",\n                \"confidence\": 0.0,\n                \"brief_description\": \"\",\n            }\n        return result\n\n    def analyze_diagram_single_pass(self, image_path: Union[str, Path]) -&gt; dict:\n        \"\"\"\n        Full single-pass diagram analysis \u2014 description, text, mermaid, chart data.\n\n        Returns parsed dict or empty dict on failure.\n        \"\"\"\n        image_bytes = _read_image_bytes(image_path)\n        raw = self.pm.analyze_image(image_bytes, _ANALYSIS_PROMPT, max_tokens=4096)\n        result = _parse_json_response(raw)\n        return result or {}\n\n    def caption_frame(self, image_path: Union[str, Path]) -&gt; str:\n        \"\"\"Get a brief caption for a screengrab fallback.\"\"\"\n        image_bytes = _read_image_bytes(image_path)\n        return self.pm.analyze_image(image_bytes, _CAPTION_PROMPT, max_tokens=256)\n\n    def process_frames(\n        self,\n        frame_paths: List[Union[str, Path]],\n        diagrams_dir: Optional[Path] = None,\n        captures_dir: Optional[Path] = None,\n    ) -&gt; Tuple[List[DiagramResult], List[ScreenCapture]]:\n        \"\"\"\n        Process a list of extracted frames: classify, analyze diagrams, screengrab fallback.\n\n        Thresholds:\n          - confidence &gt;= 0.7  \u2192 full diagram analysis (story 3.2)\n          - 0.3 &lt;= confidence &lt; 0.7 \u2192 screengrab fallback (story 3.3)\n          - confidence &lt; 0.3 \u2192 skip\n\n        Returns (diagrams, screen_captures).\n        \"\"\"\n        diagrams: List[DiagramResult] = []\n        captures: List[ScreenCapture] = []\n        diagram_idx = 0\n        capture_idx = 0\n\n        for i, fp in enumerate(tqdm(frame_paths, desc=\"Analyzing frames\", unit=\"frame\")):\n            fp = Path(fp)\n            logger.info(f\"Classifying frame {i}/{len(frame_paths)}: {fp.name}\")\n\n            try:\n                classification = self.classify_frame(fp)\n            except Exception as e:\n                logger.warning(f\"Classification failed for frame {i}: {e}\")\n                continue\n\n            confidence = float(classification.get(\"confidence\", 0.0))\n\n            if confidence &lt; self.confidence_threshold:\n                logger.debug(f\"Frame {i}: confidence {confidence:.2f} below threshold, skipping\")\n                continue\n\n            if confidence &gt;= 0.7:\n                # Full diagram analysis\n                logger.info(\n                    f\"Frame {i}: diagram detected (confidence {confidence:.2f}), analyzing...\"\n                )\n                try:\n                    analysis = self.analyze_diagram_single_pass(fp)\n                except Exception as e:\n                    logger.warning(\n                        f\"Diagram analysis failed for frame {i}: {e}, falling back to screengrab\"\n                    )\n                    analysis = {}\n\n                if not analysis:\n                    # Analysis failed \u2014 fall back to screengrab\n                    capture = self._save_screengrab(fp, i, capture_idx, captures_dir, confidence)\n                    captures.append(capture)\n                    capture_idx += 1\n                    continue\n\n                # Build DiagramResult\n                dtype = analysis.get(\"diagram_type\", classification.get(\"diagram_type\", \"unknown\"))\n                try:\n                    diagram_type = DiagramType(dtype)\n                except ValueError:\n                    diagram_type = DiagramType.unknown\n\n                dr = DiagramResult(\n                    frame_index=i,\n                    diagram_type=diagram_type,\n                    confidence=confidence,\n                    description=analysis.get(\"description\"),\n                    text_content=analysis.get(\"text_content\"),\n                    elements=analysis.get(\"elements\") or [],\n                    relationships=analysis.get(\"relationships\") or [],\n                    mermaid=analysis.get(\"mermaid\"),\n                    chart_data=analysis.get(\"chart_data\"),\n                )\n\n                # Save outputs (story 3.4)\n                if diagrams_dir:\n                    diagrams_dir.mkdir(parents=True, exist_ok=True)\n                    prefix = f\"diagram_{diagram_idx}\"\n\n                    # Original frame\n                    img_dest = diagrams_dir / f\"{prefix}.jpg\"\n                    shutil.copy2(fp, img_dest)\n                    dr.image_path = f\"diagrams/{prefix}.jpg\"\n\n                    # Mermaid source\n                    if dr.mermaid:\n                        mermaid_dest = diagrams_dir / f\"{prefix}.mermaid\"\n                        mermaid_dest.write_text(dr.mermaid)\n                        dr.mermaid_path = f\"diagrams/{prefix}.mermaid\"\n\n                    # Analysis JSON\n                    json_dest = diagrams_dir / f\"{prefix}.json\"\n                    json_dest.write_text(dr.model_dump_json(indent=2))\n\n                diagrams.append(dr)\n                diagram_idx += 1\n\n            else:\n                # Screengrab fallback (0.3 &lt;= confidence &lt; 0.7)\n                logger.info(\n                    f\"Frame {i}: uncertain (confidence {confidence:.2f}), saving as screengrab\"\n                )\n                capture = self._save_screengrab(fp, i, capture_idx, captures_dir, confidence)\n                captures.append(capture)\n                capture_idx += 1\n\n        logger.info(\n            f\"Diagram processing complete: {len(diagrams)} diagrams, {len(captures)} screengrabs\"\n        )\n        return diagrams, captures\n\n    def _save_screengrab(\n        self,\n        frame_path: Path,\n        frame_index: int,\n        capture_index: int,\n        captures_dir: Optional[Path],\n        confidence: float,\n    ) -&gt; ScreenCapture:\n        \"\"\"Save a frame as a captioned screengrab.\"\"\"\n        caption = \"\"\n        try:\n            caption = self.caption_frame(frame_path)\n        except Exception as e:\n            logger.warning(f\"Caption failed for frame {frame_index}: {e}\")\n\n        sc = ScreenCapture(\n            frame_index=frame_index,\n            caption=caption,\n            confidence=confidence,\n        )\n\n        if captures_dir:\n            captures_dir.mkdir(parents=True, exist_ok=True)\n            prefix = f\"capture_{capture_index}\"\n            img_dest = captures_dir / f\"{prefix}.jpg\"\n            shutil.copy2(frame_path, img_dest)\n            sc.image_path = f\"captures/{prefix}.jpg\"\n\n            json_dest = captures_dir / f\"{prefix}.json\"\n            json_dest.write_text(sc.model_dump_json(indent=2))\n\n        return sc\n</code></pre>"},{"location":"api/analyzers/#video_processor.analyzers.diagram_analyzer.DiagramAnalyzer.analyze_diagram_single_pass","title":"<code>analyze_diagram_single_pass(image_path)</code>","text":"<p>Full single-pass diagram analysis \u2014 description, text, mermaid, chart data.</p> <p>Returns parsed dict or empty dict on failure.</p> Source code in <code>video_processor/analyzers/diagram_analyzer.py</code> <pre><code>def analyze_diagram_single_pass(self, image_path: Union[str, Path]) -&gt; dict:\n    \"\"\"\n    Full single-pass diagram analysis \u2014 description, text, mermaid, chart data.\n\n    Returns parsed dict or empty dict on failure.\n    \"\"\"\n    image_bytes = _read_image_bytes(image_path)\n    raw = self.pm.analyze_image(image_bytes, _ANALYSIS_PROMPT, max_tokens=4096)\n    result = _parse_json_response(raw)\n    return result or {}\n</code></pre>"},{"location":"api/analyzers/#video_processor.analyzers.diagram_analyzer.DiagramAnalyzer.caption_frame","title":"<code>caption_frame(image_path)</code>","text":"<p>Get a brief caption for a screengrab fallback.</p> Source code in <code>video_processor/analyzers/diagram_analyzer.py</code> <pre><code>def caption_frame(self, image_path: Union[str, Path]) -&gt; str:\n    \"\"\"Get a brief caption for a screengrab fallback.\"\"\"\n    image_bytes = _read_image_bytes(image_path)\n    return self.pm.analyze_image(image_bytes, _CAPTION_PROMPT, max_tokens=256)\n</code></pre>"},{"location":"api/analyzers/#video_processor.analyzers.diagram_analyzer.DiagramAnalyzer.classify_frame","title":"<code>classify_frame(image_path)</code>","text":"<p>Classify a single frame using vision model.</p> <p>Returns dict with is_diagram, diagram_type, confidence, brief_description.</p> Source code in <code>video_processor/analyzers/diagram_analyzer.py</code> <pre><code>def classify_frame(self, image_path: Union[str, Path]) -&gt; dict:\n    \"\"\"\n    Classify a single frame using vision model.\n\n    Returns dict with is_diagram, diagram_type, confidence, brief_description.\n    \"\"\"\n    image_bytes = _read_image_bytes(image_path)\n    raw = self.pm.analyze_image(image_bytes, _CLASSIFY_PROMPT, max_tokens=512)\n    result = _parse_json_response(raw)\n    if result is None:\n        return {\n            \"is_diagram\": False,\n            \"diagram_type\": \"unknown\",\n            \"confidence\": 0.0,\n            \"brief_description\": \"\",\n        }\n    return result\n</code></pre>"},{"location":"api/analyzers/#video_processor.analyzers.diagram_analyzer.DiagramAnalyzer.process_frames","title":"<code>process_frames(frame_paths, diagrams_dir=None, captures_dir=None)</code>","text":"<p>Process a list of extracted frames: classify, analyze diagrams, screengrab fallback.</p> Thresholds <ul> <li>confidence &gt;= 0.7  \u2192 full diagram analysis (story 3.2)</li> <li>0.3 &lt;= confidence &lt; 0.7 \u2192 screengrab fallback (story 3.3)</li> <li>confidence &lt; 0.3 \u2192 skip</li> </ul> <p>Returns (diagrams, screen_captures).</p> Source code in <code>video_processor/analyzers/diagram_analyzer.py</code> <pre><code>def process_frames(\n    self,\n    frame_paths: List[Union[str, Path]],\n    diagrams_dir: Optional[Path] = None,\n    captures_dir: Optional[Path] = None,\n) -&gt; Tuple[List[DiagramResult], List[ScreenCapture]]:\n    \"\"\"\n    Process a list of extracted frames: classify, analyze diagrams, screengrab fallback.\n\n    Thresholds:\n      - confidence &gt;= 0.7  \u2192 full diagram analysis (story 3.2)\n      - 0.3 &lt;= confidence &lt; 0.7 \u2192 screengrab fallback (story 3.3)\n      - confidence &lt; 0.3 \u2192 skip\n\n    Returns (diagrams, screen_captures).\n    \"\"\"\n    diagrams: List[DiagramResult] = []\n    captures: List[ScreenCapture] = []\n    diagram_idx = 0\n    capture_idx = 0\n\n    for i, fp in enumerate(tqdm(frame_paths, desc=\"Analyzing frames\", unit=\"frame\")):\n        fp = Path(fp)\n        logger.info(f\"Classifying frame {i}/{len(frame_paths)}: {fp.name}\")\n\n        try:\n            classification = self.classify_frame(fp)\n        except Exception as e:\n            logger.warning(f\"Classification failed for frame {i}: {e}\")\n            continue\n\n        confidence = float(classification.get(\"confidence\", 0.0))\n\n        if confidence &lt; self.confidence_threshold:\n            logger.debug(f\"Frame {i}: confidence {confidence:.2f} below threshold, skipping\")\n            continue\n\n        if confidence &gt;= 0.7:\n            # Full diagram analysis\n            logger.info(\n                f\"Frame {i}: diagram detected (confidence {confidence:.2f}), analyzing...\"\n            )\n            try:\n                analysis = self.analyze_diagram_single_pass(fp)\n            except Exception as e:\n                logger.warning(\n                    f\"Diagram analysis failed for frame {i}: {e}, falling back to screengrab\"\n                )\n                analysis = {}\n\n            if not analysis:\n                # Analysis failed \u2014 fall back to screengrab\n                capture = self._save_screengrab(fp, i, capture_idx, captures_dir, confidence)\n                captures.append(capture)\n                capture_idx += 1\n                continue\n\n            # Build DiagramResult\n            dtype = analysis.get(\"diagram_type\", classification.get(\"diagram_type\", \"unknown\"))\n            try:\n                diagram_type = DiagramType(dtype)\n            except ValueError:\n                diagram_type = DiagramType.unknown\n\n            dr = DiagramResult(\n                frame_index=i,\n                diagram_type=diagram_type,\n                confidence=confidence,\n                description=analysis.get(\"description\"),\n                text_content=analysis.get(\"text_content\"),\n                elements=analysis.get(\"elements\") or [],\n                relationships=analysis.get(\"relationships\") or [],\n                mermaid=analysis.get(\"mermaid\"),\n                chart_data=analysis.get(\"chart_data\"),\n            )\n\n            # Save outputs (story 3.4)\n            if diagrams_dir:\n                diagrams_dir.mkdir(parents=True, exist_ok=True)\n                prefix = f\"diagram_{diagram_idx}\"\n\n                # Original frame\n                img_dest = diagrams_dir / f\"{prefix}.jpg\"\n                shutil.copy2(fp, img_dest)\n                dr.image_path = f\"diagrams/{prefix}.jpg\"\n\n                # Mermaid source\n                if dr.mermaid:\n                    mermaid_dest = diagrams_dir / f\"{prefix}.mermaid\"\n                    mermaid_dest.write_text(dr.mermaid)\n                    dr.mermaid_path = f\"diagrams/{prefix}.mermaid\"\n\n                # Analysis JSON\n                json_dest = diagrams_dir / f\"{prefix}.json\"\n                json_dest.write_text(dr.model_dump_json(indent=2))\n\n            diagrams.append(dr)\n            diagram_idx += 1\n\n        else:\n            # Screengrab fallback (0.3 &lt;= confidence &lt; 0.7)\n            logger.info(\n                f\"Frame {i}: uncertain (confidence {confidence:.2f}), saving as screengrab\"\n            )\n            capture = self._save_screengrab(fp, i, capture_idx, captures_dir, confidence)\n            captures.append(capture)\n            capture_idx += 1\n\n    logger.info(\n        f\"Diagram processing complete: {len(diagrams)} diagrams, {len(captures)} screengrabs\"\n    )\n    return diagrams, captures\n</code></pre>"},{"location":"api/analyzers/#video_processor.analyzers.content_analyzer","title":"<code>video_processor.analyzers.content_analyzer</code>","text":"<p>Content cross-referencing between transcript and diagram entities.</p>"},{"location":"api/analyzers/#video_processor.analyzers.content_analyzer.ContentAnalyzer","title":"<code>ContentAnalyzer</code>","text":"<p>Cross-references transcript and diagram entities for richer knowledge.</p> Source code in <code>video_processor/analyzers/content_analyzer.py</code> <pre><code>class ContentAnalyzer:\n    \"\"\"Cross-references transcript and diagram entities for richer knowledge.\"\"\"\n\n    def __init__(self, provider_manager: Optional[ProviderManager] = None):\n        self.pm = provider_manager\n\n    def cross_reference(\n        self,\n        transcript_entities: List[Entity],\n        diagram_entities: List[Entity],\n    ) -&gt; List[Entity]:\n        \"\"\"\n        Merge entities from transcripts and diagrams.\n\n        Merges by exact name overlap first, then uses LLM for fuzzy matching\n        of remaining entities. Adds source attribution.\n        \"\"\"\n        merged: dict[str, Entity] = {}\n\n        # Index transcript entities\n        for e in transcript_entities:\n            key = e.name.lower()\n            merged[key] = Entity(\n                name=e.name,\n                type=e.type,\n                descriptions=list(e.descriptions),\n                source=\"transcript\",\n                occurrences=list(e.occurrences),\n            )\n\n        # Merge diagram entities\n        for e in diagram_entities:\n            key = e.name.lower()\n            if key in merged:\n                existing = merged[key]\n                existing.source = \"both\"\n                existing.descriptions = list(set(existing.descriptions + e.descriptions))\n                existing.occurrences.extend(e.occurrences)\n            else:\n                merged[key] = Entity(\n                    name=e.name,\n                    type=e.type,\n                    descriptions=list(e.descriptions),\n                    source=\"diagram\",\n                    occurrences=list(e.occurrences),\n                )\n\n        # LLM fuzzy matching for unmatched entities\n        if self.pm:\n            unmatched_t = [\n                e\n                for e in transcript_entities\n                if e.name.lower() not in {d.name.lower() for d in diagram_entities}\n            ]\n            unmatched_d = [\n                e\n                for e in diagram_entities\n                if e.name.lower() not in {t.name.lower() for t in transcript_entities}\n            ]\n\n            if unmatched_t and unmatched_d:\n                matches = self._fuzzy_match(unmatched_t, unmatched_d)\n                for t_name, d_name in matches:\n                    t_key = t_name.lower()\n                    d_key = d_name.lower()\n                    if t_key in merged and d_key in merged:\n                        t_entity = merged[t_key]\n                        d_entity = merged.pop(d_key)\n                        t_entity.source = \"both\"\n                        t_entity.descriptions = list(\n                            set(t_entity.descriptions + d_entity.descriptions)\n                        )\n                        t_entity.occurrences.extend(d_entity.occurrences)\n\n        return list(merged.values())\n\n    def _fuzzy_match(\n        self,\n        transcript_entities: List[Entity],\n        diagram_entities: List[Entity],\n    ) -&gt; List[tuple[str, str]]:\n        \"\"\"Use LLM to fuzzy-match entity names across sources.\"\"\"\n        if not self.pm:\n            return []\n\n        t_names = [e.name for e in transcript_entities]\n        d_names = [e.name for e in diagram_entities]\n\n        prompt = (\n            \"Match entities that refer to the same thing across these two lists.\\n\\n\"\n            f\"Transcript entities: {t_names}\\n\"\n            f\"Diagram entities: {d_names}\\n\\n\"\n            \"Return a JSON array of matched pairs:\\n\"\n            '[{\"transcript\": \"name from list 1\", \"diagram\": \"name from list 2\"}]\\n\\n'\n            \"Only include confident matches. Return empty array if no matches.\\n\"\n            \"Return ONLY the JSON array.\"\n        )\n\n        try:\n            raw = self.pm.chat([{\"role\": \"user\", \"content\": prompt}], temperature=0.2)\n            parsed = parse_json_from_response(raw)\n            if isinstance(parsed, list):\n                return [\n                    (item[\"transcript\"], item[\"diagram\"])\n                    for item in parsed\n                    if isinstance(item, dict) and \"transcript\" in item and \"diagram\" in item\n                ]\n        except Exception as e:\n            logger.warning(f\"Fuzzy matching failed: {e}\")\n\n        return []\n\n    def enrich_key_points(\n        self,\n        key_points: List[KeyPoint],\n        diagrams: list,\n        transcript_text: str,\n    ) -&gt; List[KeyPoint]:\n        \"\"\"\n        Link key points to relevant diagrams by entity overlap and temporal proximity.\n        \"\"\"\n        if not diagrams:\n            return key_points\n\n        # Build diagram entity index\n        diagram_entities: dict[int, set[str]] = {}\n        for i, d in enumerate(diagrams):\n            elements = d.get(\"elements\", []) if isinstance(d, dict) else getattr(d, \"elements\", [])\n            text = (\n                d.get(\"text_content\", \"\") if isinstance(d, dict) else getattr(d, \"text_content\", \"\")\n            )\n            entities = set(str(e).lower() for e in elements)\n            if text:\n                entities.update(word.lower() for word in text.split() if len(word) &gt; 3)\n            diagram_entities[i] = entities\n\n        # Match key points to diagrams\n        for kp in key_points:\n            kp_words = set(kp.point.lower().split())\n            if kp.details:\n                kp_words.update(kp.details.lower().split())\n\n            related = []\n            for idx, d_entities in diagram_entities.items():\n                overlap = kp_words &amp; d_entities\n                if len(overlap) &gt;= 2:\n                    related.append(idx)\n\n            if related:\n                kp.related_diagrams = related\n\n        return key_points\n</code></pre>"},{"location":"api/analyzers/#video_processor.analyzers.content_analyzer.ContentAnalyzer.cross_reference","title":"<code>cross_reference(transcript_entities, diagram_entities)</code>","text":"<p>Merge entities from transcripts and diagrams.</p> <p>Merges by exact name overlap first, then uses LLM for fuzzy matching of remaining entities. Adds source attribution.</p> Source code in <code>video_processor/analyzers/content_analyzer.py</code> <pre><code>def cross_reference(\n    self,\n    transcript_entities: List[Entity],\n    diagram_entities: List[Entity],\n) -&gt; List[Entity]:\n    \"\"\"\n    Merge entities from transcripts and diagrams.\n\n    Merges by exact name overlap first, then uses LLM for fuzzy matching\n    of remaining entities. Adds source attribution.\n    \"\"\"\n    merged: dict[str, Entity] = {}\n\n    # Index transcript entities\n    for e in transcript_entities:\n        key = e.name.lower()\n        merged[key] = Entity(\n            name=e.name,\n            type=e.type,\n            descriptions=list(e.descriptions),\n            source=\"transcript\",\n            occurrences=list(e.occurrences),\n        )\n\n    # Merge diagram entities\n    for e in diagram_entities:\n        key = e.name.lower()\n        if key in merged:\n            existing = merged[key]\n            existing.source = \"both\"\n            existing.descriptions = list(set(existing.descriptions + e.descriptions))\n            existing.occurrences.extend(e.occurrences)\n        else:\n            merged[key] = Entity(\n                name=e.name,\n                type=e.type,\n                descriptions=list(e.descriptions),\n                source=\"diagram\",\n                occurrences=list(e.occurrences),\n            )\n\n    # LLM fuzzy matching for unmatched entities\n    if self.pm:\n        unmatched_t = [\n            e\n            for e in transcript_entities\n            if e.name.lower() not in {d.name.lower() for d in diagram_entities}\n        ]\n        unmatched_d = [\n            e\n            for e in diagram_entities\n            if e.name.lower() not in {t.name.lower() for t in transcript_entities}\n        ]\n\n        if unmatched_t and unmatched_d:\n            matches = self._fuzzy_match(unmatched_t, unmatched_d)\n            for t_name, d_name in matches:\n                t_key = t_name.lower()\n                d_key = d_name.lower()\n                if t_key in merged and d_key in merged:\n                    t_entity = merged[t_key]\n                    d_entity = merged.pop(d_key)\n                    t_entity.source = \"both\"\n                    t_entity.descriptions = list(\n                        set(t_entity.descriptions + d_entity.descriptions)\n                    )\n                    t_entity.occurrences.extend(d_entity.occurrences)\n\n    return list(merged.values())\n</code></pre>"},{"location":"api/analyzers/#video_processor.analyzers.content_analyzer.ContentAnalyzer.enrich_key_points","title":"<code>enrich_key_points(key_points, diagrams, transcript_text)</code>","text":"<p>Link key points to relevant diagrams by entity overlap and temporal proximity.</p> Source code in <code>video_processor/analyzers/content_analyzer.py</code> <pre><code>def enrich_key_points(\n    self,\n    key_points: List[KeyPoint],\n    diagrams: list,\n    transcript_text: str,\n) -&gt; List[KeyPoint]:\n    \"\"\"\n    Link key points to relevant diagrams by entity overlap and temporal proximity.\n    \"\"\"\n    if not diagrams:\n        return key_points\n\n    # Build diagram entity index\n    diagram_entities: dict[int, set[str]] = {}\n    for i, d in enumerate(diagrams):\n        elements = d.get(\"elements\", []) if isinstance(d, dict) else getattr(d, \"elements\", [])\n        text = (\n            d.get(\"text_content\", \"\") if isinstance(d, dict) else getattr(d, \"text_content\", \"\")\n        )\n        entities = set(str(e).lower() for e in elements)\n        if text:\n            entities.update(word.lower() for word in text.split() if len(word) &gt; 3)\n        diagram_entities[i] = entities\n\n    # Match key points to diagrams\n    for kp in key_points:\n        kp_words = set(kp.point.lower().split())\n        if kp.details:\n            kp_words.update(kp.details.lower().split())\n\n        related = []\n        for idx, d_entities in diagram_entities.items():\n            overlap = kp_words &amp; d_entities\n            if len(overlap) &gt;= 2:\n                related.append(idx)\n\n        if related:\n            kp.related_diagrams = related\n\n    return key_points\n</code></pre>"},{"location":"api/analyzers/#video_processor.analyzers.action_detector","title":"<code>video_processor.analyzers.action_detector</code>","text":"<p>Enhanced action item detection from transcripts and diagrams.</p>"},{"location":"api/analyzers/#video_processor.analyzers.action_detector.ActionDetector","title":"<code>ActionDetector</code>","text":"<p>Detects action items from transcripts using heuristics and LLM.</p> Source code in <code>video_processor/analyzers/action_detector.py</code> <pre><code>class ActionDetector:\n    \"\"\"Detects action items from transcripts using heuristics and LLM.\"\"\"\n\n    def __init__(self, provider_manager: Optional[ProviderManager] = None):\n        self.pm = provider_manager\n\n    def detect_from_transcript(\n        self,\n        text: str,\n        segments: Optional[List[TranscriptSegment]] = None,\n    ) -&gt; List[ActionItem]:\n        \"\"\"\n        Detect action items from transcript text.\n\n        Uses LLM extraction when available, falls back to pattern matching.\n        Segments are used to attach timestamps.\n        \"\"\"\n        if self.pm:\n            items = self._llm_extract(text)\n        else:\n            items = self._pattern_extract(text)\n\n        # Attach timestamps from segments if available\n        if segments and items:\n            self._attach_timestamps(items, segments)\n\n        return items\n\n    def detect_from_diagrams(\n        self,\n        diagrams: list,\n    ) -&gt; List[ActionItem]:\n        \"\"\"\n        Extract action items mentioned in diagram text content.\n\n        Looks for action-oriented language in diagram text/elements.\n        \"\"\"\n        items: List[ActionItem] = []\n\n        for diagram in diagrams:\n            text = \"\"\n            if isinstance(diagram, dict):\n                text = diagram.get(\"text_content\", \"\") or \"\"\n                elements = diagram.get(\"elements\", [])\n            else:\n                text = getattr(diagram, \"text_content\", \"\") or \"\"\n                elements = getattr(diagram, \"elements\", [])\n\n            combined = text + \" \" + \" \".join(str(e) for e in elements)\n            if not combined.strip():\n                continue\n\n            if self.pm:\n                diagram_items = self._llm_extract(combined)\n            else:\n                diagram_items = self._pattern_extract(combined)\n\n            for item in diagram_items:\n                item.source = \"diagram\"\n            items.extend(diagram_items)\n\n        return items\n\n    def merge_action_items(\n        self,\n        transcript_items: List[ActionItem],\n        diagram_items: List[ActionItem],\n    ) -&gt; List[ActionItem]:\n        \"\"\"\n        Merge action items from transcript and diagram sources.\n\n        Deduplicates by checking for similar action text.\n        \"\"\"\n        merged: List[ActionItem] = list(transcript_items)\n        existing_actions = {a.action.lower().strip() for a in merged}\n\n        for item in diagram_items:\n            normalized = item.action.lower().strip()\n            if normalized not in existing_actions:\n                merged.append(item)\n                existing_actions.add(normalized)\n\n        return merged\n\n    def _llm_extract(self, text: str) -&gt; List[ActionItem]:\n        \"\"\"Extract action items using LLM.\"\"\"\n        if not self.pm:\n            return []\n\n        prompt = (\n            \"Extract all action items, tasks, and commitments \"\n            \"from the following text.\\n\\n\"\n            f\"TEXT:\\n{text[:8000]}\\n\\n\"\n            \"Return a JSON array:\\n\"\n            '[{\"action\": \"...\", \"assignee\": \"...\", \"deadline\": \"...\", '\n            '\"priority\": \"...\", \"context\": \"...\"}]\\n\\n'\n            \"Only include clear, actionable items. \"\n            \"Set fields to null if not mentioned.\\n\"\n            \"Return ONLY the JSON array.\"\n        )\n\n        try:\n            raw = self.pm.chat(\n                [{\"role\": \"user\", \"content\": prompt}],\n                temperature=0.3,\n            )\n            parsed = parse_json_from_response(raw)\n            if isinstance(parsed, list):\n                return [\n                    ActionItem(\n                        action=item.get(\"action\", \"\"),\n                        assignee=item.get(\"assignee\"),\n                        deadline=item.get(\"deadline\"),\n                        priority=item.get(\"priority\"),\n                        context=item.get(\"context\"),\n                        source=\"transcript\",\n                    )\n                    for item in parsed\n                    if isinstance(item, dict) and item.get(\"action\")\n                ]\n        except Exception as e:\n            logger.warning(f\"LLM action extraction failed: {e}\")\n\n        return []\n\n    def _pattern_extract(self, text: str) -&gt; List[ActionItem]:\n        \"\"\"Extract action items using regex pattern matching.\"\"\"\n        items: List[ActionItem] = []\n        sentences = re.split(r\"[.!?]\\s+\", text)\n\n        for sentence in sentences:\n            sentence = sentence.strip()\n            if not sentence or len(sentence) &lt; 10:\n                continue\n\n            for pattern in _ACTION_PATTERNS:\n                if pattern.search(sentence):\n                    items.append(\n                        ActionItem(\n                            action=sentence,\n                            source=\"transcript\",\n                        )\n                    )\n                    break  # One match per sentence is enough\n\n        return items\n\n    def _attach_timestamps(\n        self,\n        items: List[ActionItem],\n        segments: List[TranscriptSegment],\n    ) -&gt; None:\n        \"\"\"Attach timestamps to action items by finding matching segments.\"\"\"\n        for item in items:\n            action_lower = item.action.lower()\n            best_overlap = 0\n            best_segment = None\n\n            for seg in segments:\n                seg_lower = seg.text.lower()\n                # Check word overlap\n                action_words = set(action_lower.split())\n                seg_words = set(seg_lower.split())\n                overlap = len(action_words &amp; seg_words)\n\n                if overlap &gt; best_overlap:\n                    best_overlap = overlap\n                    best_segment = seg\n\n            if best_segment and best_overlap &gt;= 3:\n                if not item.context:\n                    item.context = f\"at {best_segment.start:.0f}s\"\n</code></pre>"},{"location":"api/analyzers/#video_processor.analyzers.action_detector.ActionDetector.detect_from_diagrams","title":"<code>detect_from_diagrams(diagrams)</code>","text":"<p>Extract action items mentioned in diagram text content.</p> <p>Looks for action-oriented language in diagram text/elements.</p> Source code in <code>video_processor/analyzers/action_detector.py</code> <pre><code>def detect_from_diagrams(\n    self,\n    diagrams: list,\n) -&gt; List[ActionItem]:\n    \"\"\"\n    Extract action items mentioned in diagram text content.\n\n    Looks for action-oriented language in diagram text/elements.\n    \"\"\"\n    items: List[ActionItem] = []\n\n    for diagram in diagrams:\n        text = \"\"\n        if isinstance(diagram, dict):\n            text = diagram.get(\"text_content\", \"\") or \"\"\n            elements = diagram.get(\"elements\", [])\n        else:\n            text = getattr(diagram, \"text_content\", \"\") or \"\"\n            elements = getattr(diagram, \"elements\", [])\n\n        combined = text + \" \" + \" \".join(str(e) for e in elements)\n        if not combined.strip():\n            continue\n\n        if self.pm:\n            diagram_items = self._llm_extract(combined)\n        else:\n            diagram_items = self._pattern_extract(combined)\n\n        for item in diagram_items:\n            item.source = \"diagram\"\n        items.extend(diagram_items)\n\n    return items\n</code></pre>"},{"location":"api/analyzers/#video_processor.analyzers.action_detector.ActionDetector.detect_from_transcript","title":"<code>detect_from_transcript(text, segments=None)</code>","text":"<p>Detect action items from transcript text.</p> <p>Uses LLM extraction when available, falls back to pattern matching. Segments are used to attach timestamps.</p> Source code in <code>video_processor/analyzers/action_detector.py</code> <pre><code>def detect_from_transcript(\n    self,\n    text: str,\n    segments: Optional[List[TranscriptSegment]] = None,\n) -&gt; List[ActionItem]:\n    \"\"\"\n    Detect action items from transcript text.\n\n    Uses LLM extraction when available, falls back to pattern matching.\n    Segments are used to attach timestamps.\n    \"\"\"\n    if self.pm:\n        items = self._llm_extract(text)\n    else:\n        items = self._pattern_extract(text)\n\n    # Attach timestamps from segments if available\n    if segments and items:\n        self._attach_timestamps(items, segments)\n\n    return items\n</code></pre>"},{"location":"api/analyzers/#video_processor.analyzers.action_detector.ActionDetector.merge_action_items","title":"<code>merge_action_items(transcript_items, diagram_items)</code>","text":"<p>Merge action items from transcript and diagram sources.</p> <p>Deduplicates by checking for similar action text.</p> Source code in <code>video_processor/analyzers/action_detector.py</code> <pre><code>def merge_action_items(\n    self,\n    transcript_items: List[ActionItem],\n    diagram_items: List[ActionItem],\n) -&gt; List[ActionItem]:\n    \"\"\"\n    Merge action items from transcript and diagram sources.\n\n    Deduplicates by checking for similar action text.\n    \"\"\"\n    merged: List[ActionItem] = list(transcript_items)\n    existing_actions = {a.action.lower().strip() for a in merged}\n\n    for item in diagram_items:\n        normalized = item.action.lower().strip()\n        if normalized not in existing_actions:\n            merged.append(item)\n            existing_actions.add(normalized)\n\n    return merged\n</code></pre>"},{"location":"api/models/","title":"Models API Reference","text":""},{"location":"api/models/#video_processor.models","title":"<code>video_processor.models</code>","text":"<p>Pydantic data models for PlanOpticon output.</p>"},{"location":"api/models/#video_processor.models.ActionItem","title":"<code>ActionItem</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>An action item extracted from content.</p> Source code in <code>video_processor/models.py</code> <pre><code>class ActionItem(BaseModel):\n    \"\"\"An action item extracted from content.\"\"\"\n\n    action: str = Field(description=\"The action to be taken\")\n    assignee: Optional[str] = Field(default=None, description=\"Person responsible\")\n    deadline: Optional[str] = Field(default=None, description=\"Deadline or timeframe\")\n    priority: Optional[str] = Field(default=None, description=\"Priority level\")\n    context: Optional[str] = Field(default=None, description=\"Additional context\")\n    source: Optional[str] = Field(\n        default=None, description=\"Where this was found (transcript/diagram)\"\n    )\n</code></pre>"},{"location":"api/models/#video_processor.models.BatchManifest","title":"<code>BatchManifest</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Manifest for a batch processing run.</p> Source code in <code>video_processor/models.py</code> <pre><code>class BatchManifest(BaseModel):\n    \"\"\"Manifest for a batch processing run.\"\"\"\n\n    version: str = Field(default=\"1.0\")\n    title: str = Field(default=\"Batch Processing Results\")\n    processed_at: str = Field(default_factory=lambda: datetime.now().isoformat())\n    stats: ProcessingStats = Field(default_factory=ProcessingStats)\n\n    videos: List[BatchVideoEntry] = Field(default_factory=list)\n\n    # Aggregated counts\n    total_videos: int = Field(default=0)\n    completed_videos: int = Field(default=0)\n    failed_videos: int = Field(default=0)\n    total_diagrams: int = Field(default=0)\n    total_action_items: int = Field(default=0)\n    total_key_points: int = Field(default=0)\n\n    # Batch-level output paths (relative)\n    batch_summary_md: Optional[str] = Field(default=None)\n    merged_knowledge_graph_json: Optional[str] = Field(default=None)\n</code></pre>"},{"location":"api/models/#video_processor.models.BatchVideoEntry","title":"<code>BatchVideoEntry</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Summary of a single video within a batch.</p> Source code in <code>video_processor/models.py</code> <pre><code>class BatchVideoEntry(BaseModel):\n    \"\"\"Summary of a single video within a batch.\"\"\"\n\n    video_name: str\n    manifest_path: str = Field(description=\"Relative path to video manifest\")\n    status: str = Field(default=\"pending\", description=\"pending/completed/failed\")\n    error: Optional[str] = Field(default=None, description=\"Error message if failed\")\n    diagrams_count: int = Field(default=0)\n    action_items_count: int = Field(default=0)\n    key_points_count: int = Field(default=0)\n    duration_seconds: Optional[float] = Field(default=None)\n</code></pre>"},{"location":"api/models/#video_processor.models.DiagramResult","title":"<code>DiagramResult</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Result from diagram extraction and analysis.</p> Source code in <code>video_processor/models.py</code> <pre><code>class DiagramResult(BaseModel):\n    \"\"\"Result from diagram extraction and analysis.\"\"\"\n\n    frame_index: int = Field(description=\"Index of the source frame\")\n    timestamp: Optional[float] = Field(default=None, description=\"Timestamp in video (seconds)\")\n    diagram_type: DiagramType = Field(default=DiagramType.unknown, description=\"Type of diagram\")\n    confidence: float = Field(default=0.0, description=\"Detection confidence 0-1\")\n    description: Optional[str] = Field(default=None, description=\"Description of the diagram\")\n    text_content: Optional[str] = Field(default=None, description=\"Text visible in the diagram\")\n    elements: List[str] = Field(default_factory=list, description=\"Identified elements\")\n    relationships: List[str] = Field(default_factory=list, description=\"Identified relationships\")\n    mermaid: Optional[str] = Field(default=None, description=\"Mermaid syntax representation\")\n    chart_data: Optional[Dict[str, Any]] = Field(\n        default=None, description=\"Chart data for reproduction (labels, values, chart_type)\"\n    )\n    image_path: Optional[str] = Field(default=None, description=\"Relative path to original frame\")\n    svg_path: Optional[str] = Field(default=None, description=\"Relative path to rendered SVG\")\n    png_path: Optional[str] = Field(default=None, description=\"Relative path to rendered PNG\")\n    mermaid_path: Optional[str] = Field(default=None, description=\"Relative path to mermaid source\")\n</code></pre>"},{"location":"api/models/#video_processor.models.DiagramType","title":"<code>DiagramType</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Types of visual content detected in video frames.</p> Source code in <code>video_processor/models.py</code> <pre><code>class DiagramType(str, Enum):\n    \"\"\"Types of visual content detected in video frames.\"\"\"\n\n    flowchart = \"flowchart\"\n    sequence = \"sequence\"\n    architecture = \"architecture\"\n    whiteboard = \"whiteboard\"\n    chart = \"chart\"\n    table = \"table\"\n    slide = \"slide\"\n    screenshot = \"screenshot\"\n    unknown = \"unknown\"\n</code></pre>"},{"location":"api/models/#video_processor.models.Entity","title":"<code>Entity</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>An entity in the knowledge graph.</p> Source code in <code>video_processor/models.py</code> <pre><code>class Entity(BaseModel):\n    \"\"\"An entity in the knowledge graph.\"\"\"\n\n    name: str = Field(description=\"Entity name\")\n    type: str = Field(default=\"concept\", description=\"Entity type (person, concept, time, diagram)\")\n    descriptions: List[str] = Field(default_factory=list, description=\"Descriptions of this entity\")\n    source: Optional[str] = Field(\n        default=None, description=\"Source attribution (transcript/diagram/both)\"\n    )\n    occurrences: List[Dict[str, Any]] = Field(\n        default_factory=list, description=\"List of occurrences with source, timestamp, text\"\n    )\n</code></pre>"},{"location":"api/models/#video_processor.models.KeyPoint","title":"<code>KeyPoint</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A key point extracted from content.</p> Source code in <code>video_processor/models.py</code> <pre><code>class KeyPoint(BaseModel):\n    \"\"\"A key point extracted from content.\"\"\"\n\n    point: str = Field(description=\"The key point\")\n    topic: Optional[str] = Field(default=None, description=\"Topic or category\")\n    details: Optional[str] = Field(default=None, description=\"Supporting details\")\n    timestamp: Optional[float] = Field(default=None, description=\"Timestamp in video (seconds)\")\n    source: Optional[str] = Field(default=None, description=\"Where this was found\")\n    related_diagrams: List[int] = Field(\n        default_factory=list, description=\"Indices of related diagrams\"\n    )\n</code></pre>"},{"location":"api/models/#video_processor.models.KnowledgeGraphData","title":"<code>KnowledgeGraphData</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Serializable knowledge graph data.</p> Source code in <code>video_processor/models.py</code> <pre><code>class KnowledgeGraphData(BaseModel):\n    \"\"\"Serializable knowledge graph data.\"\"\"\n\n    nodes: List[Entity] = Field(default_factory=list, description=\"Graph nodes/entities\")\n    relationships: List[Relationship] = Field(\n        default_factory=list, description=\"Graph relationships\"\n    )\n</code></pre>"},{"location":"api/models/#video_processor.models.OutputFormat","title":"<code>OutputFormat</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Available output formats.</p> Source code in <code>video_processor/models.py</code> <pre><code>class OutputFormat(str, Enum):\n    \"\"\"Available output formats.\"\"\"\n\n    markdown = \"markdown\"\n    json = \"json\"\n    html = \"html\"\n    pdf = \"pdf\"\n    svg = \"svg\"\n    png = \"png\"\n</code></pre>"},{"location":"api/models/#video_processor.models.ProcessingStats","title":"<code>ProcessingStats</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Statistics about a processing run.</p> Source code in <code>video_processor/models.py</code> <pre><code>class ProcessingStats(BaseModel):\n    \"\"\"Statistics about a processing run.\"\"\"\n\n    start_time: Optional[str] = Field(default=None, description=\"ISO format start time\")\n    end_time: Optional[str] = Field(default=None, description=\"ISO format end time\")\n    duration_seconds: Optional[float] = Field(default=None, description=\"Total processing time\")\n    frames_extracted: int = Field(default=0)\n    people_frames_filtered: int = Field(default=0)\n    diagrams_detected: int = Field(default=0)\n    screen_captures: int = Field(default=0)\n    transcript_duration_seconds: Optional[float] = Field(default=None)\n    models_used: Dict[str, str] = Field(\n        default_factory=dict, description=\"Map of task to model used (e.g. vision: gpt-4o)\"\n    )\n</code></pre>"},{"location":"api/models/#video_processor.models.Relationship","title":"<code>Relationship</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A relationship between entities in the knowledge graph.</p> Source code in <code>video_processor/models.py</code> <pre><code>class Relationship(BaseModel):\n    \"\"\"A relationship between entities in the knowledge graph.\"\"\"\n\n    source: str = Field(description=\"Source entity name\")\n    target: str = Field(description=\"Target entity name\")\n    type: str = Field(default=\"related_to\", description=\"Relationship type\")\n    content_source: Optional[str] = Field(default=None, description=\"Content source identifier\")\n    timestamp: Optional[float] = Field(default=None, description=\"Timestamp in seconds\")\n</code></pre>"},{"location":"api/models/#video_processor.models.ScreenCapture","title":"<code>ScreenCapture</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A screengrab fallback when diagram extraction fails or is uncertain.</p> Source code in <code>video_processor/models.py</code> <pre><code>class ScreenCapture(BaseModel):\n    \"\"\"A screengrab fallback when diagram extraction fails or is uncertain.\"\"\"\n\n    frame_index: int = Field(description=\"Index of the source frame\")\n    timestamp: Optional[float] = Field(default=None, description=\"Timestamp in video (seconds)\")\n    caption: Optional[str] = Field(default=None, description=\"Brief description of the content\")\n    image_path: Optional[str] = Field(default=None, description=\"Relative path to screenshot\")\n    confidence: float = Field(\n        default=0.0, description=\"Detection confidence that triggered fallback\"\n    )\n</code></pre>"},{"location":"api/models/#video_processor.models.TranscriptSegment","title":"<code>TranscriptSegment</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A single segment of transcribed audio.</p> Source code in <code>video_processor/models.py</code> <pre><code>class TranscriptSegment(BaseModel):\n    \"\"\"A single segment of transcribed audio.\"\"\"\n\n    start: float = Field(description=\"Start time in seconds\")\n    end: float = Field(description=\"End time in seconds\")\n    text: str = Field(description=\"Transcribed text\")\n    speaker: Optional[str] = Field(default=None, description=\"Speaker identifier\")\n    confidence: Optional[float] = Field(default=None, description=\"Transcription confidence 0-1\")\n</code></pre>"},{"location":"api/models/#video_processor.models.VideoManifest","title":"<code>VideoManifest</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Manifest for a single video processing run - the single source of truth.</p> Source code in <code>video_processor/models.py</code> <pre><code>class VideoManifest(BaseModel):\n    \"\"\"Manifest for a single video processing run - the single source of truth.\"\"\"\n\n    version: str = Field(default=\"1.0\", description=\"Manifest schema version\")\n    video: VideoMetadata = Field(description=\"Source video metadata\")\n    stats: ProcessingStats = Field(default_factory=ProcessingStats)\n\n    # Relative paths to output files\n    transcript_json: Optional[str] = Field(default=None)\n    transcript_txt: Optional[str] = Field(default=None)\n    transcript_srt: Optional[str] = Field(default=None)\n    analysis_md: Optional[str] = Field(default=None)\n    analysis_html: Optional[str] = Field(default=None)\n    analysis_pdf: Optional[str] = Field(default=None)\n    knowledge_graph_json: Optional[str] = Field(default=None)\n    key_points_json: Optional[str] = Field(default=None)\n    action_items_json: Optional[str] = Field(default=None)\n\n    # Inline structured data\n    key_points: List[KeyPoint] = Field(default_factory=list)\n    action_items: List[ActionItem] = Field(default_factory=list)\n    diagrams: List[DiagramResult] = Field(default_factory=list)\n    screen_captures: List[ScreenCapture] = Field(default_factory=list)\n\n    # Frame paths\n    frame_paths: List[str] = Field(\n        default_factory=list, description=\"Relative paths to extracted frames\"\n    )\n</code></pre>"},{"location":"api/models/#video_processor.models.VideoMetadata","title":"<code>VideoMetadata</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Metadata about the source video.</p> Source code in <code>video_processor/models.py</code> <pre><code>class VideoMetadata(BaseModel):\n    \"\"\"Metadata about the source video.\"\"\"\n\n    title: str = Field(description=\"Video title\")\n    source_path: Optional[str] = Field(default=None, description=\"Original video file path\")\n    duration_seconds: Optional[float] = Field(default=None, description=\"Video duration\")\n    resolution: Optional[str] = Field(default=None, description=\"Video resolution (e.g. 1920x1080)\")\n    processed_at: str = Field(\n        default_factory=lambda: datetime.now().isoformat(),\n        description=\"ISO format processing timestamp\",\n    )\n</code></pre>"},{"location":"api/providers/","title":"Providers API Reference","text":""},{"location":"api/providers/#video_processor.providers.base","title":"<code>video_processor.providers.base</code>","text":"<p>Abstract base class and shared types for provider implementations.</p>"},{"location":"api/providers/#video_processor.providers.base.BaseProvider","title":"<code>BaseProvider</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base for all provider implementations.</p> Source code in <code>video_processor/providers/base.py</code> <pre><code>class BaseProvider(ABC):\n    \"\"\"Abstract base for all provider implementations.\"\"\"\n\n    provider_name: str = \"\"\n\n    @abstractmethod\n    def chat(\n        self,\n        messages: list[dict],\n        max_tokens: int = 4096,\n        temperature: float = 0.7,\n        model: Optional[str] = None,\n    ) -&gt; str:\n        \"\"\"Send a chat completion request. Returns the assistant text.\"\"\"\n\n    @abstractmethod\n    def analyze_image(\n        self,\n        image_bytes: bytes,\n        prompt: str,\n        max_tokens: int = 4096,\n        model: Optional[str] = None,\n    ) -&gt; str:\n        \"\"\"Analyze an image with a prompt. Returns the assistant text.\"\"\"\n\n    @abstractmethod\n    def transcribe_audio(\n        self,\n        audio_path: str | Path,\n        language: Optional[str] = None,\n        model: Optional[str] = None,\n    ) -&gt; dict:\n        \"\"\"Transcribe an audio file. Returns dict with 'text', 'segments', etc.\"\"\"\n\n    @abstractmethod\n    def list_models(self) -&gt; list[ModelInfo]:\n        \"\"\"Discover available models from this provider's API.\"\"\"\n</code></pre>"},{"location":"api/providers/#video_processor.providers.base.BaseProvider.analyze_image","title":"<code>analyze_image(image_bytes, prompt, max_tokens=4096, model=None)</code>  <code>abstractmethod</code>","text":"<p>Analyze an image with a prompt. Returns the assistant text.</p> Source code in <code>video_processor/providers/base.py</code> <pre><code>@abstractmethod\ndef analyze_image(\n    self,\n    image_bytes: bytes,\n    prompt: str,\n    max_tokens: int = 4096,\n    model: Optional[str] = None,\n) -&gt; str:\n    \"\"\"Analyze an image with a prompt. Returns the assistant text.\"\"\"\n</code></pre>"},{"location":"api/providers/#video_processor.providers.base.BaseProvider.chat","title":"<code>chat(messages, max_tokens=4096, temperature=0.7, model=None)</code>  <code>abstractmethod</code>","text":"<p>Send a chat completion request. Returns the assistant text.</p> Source code in <code>video_processor/providers/base.py</code> <pre><code>@abstractmethod\ndef chat(\n    self,\n    messages: list[dict],\n    max_tokens: int = 4096,\n    temperature: float = 0.7,\n    model: Optional[str] = None,\n) -&gt; str:\n    \"\"\"Send a chat completion request. Returns the assistant text.\"\"\"\n</code></pre>"},{"location":"api/providers/#video_processor.providers.base.BaseProvider.list_models","title":"<code>list_models()</code>  <code>abstractmethod</code>","text":"<p>Discover available models from this provider's API.</p> Source code in <code>video_processor/providers/base.py</code> <pre><code>@abstractmethod\ndef list_models(self) -&gt; list[ModelInfo]:\n    \"\"\"Discover available models from this provider's API.\"\"\"\n</code></pre>"},{"location":"api/providers/#video_processor.providers.base.BaseProvider.transcribe_audio","title":"<code>transcribe_audio(audio_path, language=None, model=None)</code>  <code>abstractmethod</code>","text":"<p>Transcribe an audio file. Returns dict with 'text', 'segments', etc.</p> Source code in <code>video_processor/providers/base.py</code> <pre><code>@abstractmethod\ndef transcribe_audio(\n    self,\n    audio_path: str | Path,\n    language: Optional[str] = None,\n    model: Optional[str] = None,\n) -&gt; dict:\n    \"\"\"Transcribe an audio file. Returns dict with 'text', 'segments', etc.\"\"\"\n</code></pre>"},{"location":"api/providers/#video_processor.providers.base.ModelInfo","title":"<code>ModelInfo</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Information about an available model.</p> Source code in <code>video_processor/providers/base.py</code> <pre><code>class ModelInfo(BaseModel):\n    \"\"\"Information about an available model.\"\"\"\n\n    id: str = Field(description=\"Model identifier (e.g. gpt-4o)\")\n    provider: str = Field(description=\"Provider name (openai, anthropic, gemini)\")\n    display_name: str = Field(default=\"\", description=\"Human-readable name\")\n    capabilities: List[str] = Field(\n        default_factory=list, description=\"Model capabilities: chat, vision, audio, embedding\"\n    )\n</code></pre>"},{"location":"api/providers/#video_processor.providers.manager","title":"<code>video_processor.providers.manager</code>","text":"<p>ProviderManager - unified interface for routing API calls to the best available provider.</p>"},{"location":"api/providers/#video_processor.providers.manager.ProviderManager","title":"<code>ProviderManager</code>","text":"<p>Routes API calls to the best available provider/model.</p> <p>Supports explicit model selection or auto-routing based on discovered available models.</p> Source code in <code>video_processor/providers/manager.py</code> <pre><code>class ProviderManager:\n    \"\"\"\n    Routes API calls to the best available provider/model.\n\n    Supports explicit model selection or auto-routing based on\n    discovered available models.\n    \"\"\"\n\n    def __init__(\n        self,\n        vision_model: Optional[str] = None,\n        chat_model: Optional[str] = None,\n        transcription_model: Optional[str] = None,\n        provider: Optional[str] = None,\n        auto: bool = True,\n    ):\n        \"\"\"\n        Initialize the ProviderManager.\n\n        Parameters\n        ----------\n        vision_model : override model for vision tasks (e.g. 'gpt-4o')\n        chat_model : override model for chat/LLM tasks\n        transcription_model : override model for transcription\n        provider : force all tasks to a single provider ('openai', 'anthropic', 'gemini')\n        auto : if True and no model specified, pick the best available\n        \"\"\"\n        self.auto = auto\n        self._providers: dict[str, BaseProvider] = {}\n        self._available_models: Optional[list[ModelInfo]] = None\n        self.usage = UsageTracker()\n\n        # If a single provider is forced, apply it\n        if provider:\n            self.vision_model = vision_model or self._default_for_provider(provider, \"vision\")\n            self.chat_model = chat_model or self._default_for_provider(provider, \"chat\")\n            self.transcription_model = transcription_model or self._default_for_provider(\n                provider, \"audio\"\n            )\n        else:\n            self.vision_model = vision_model\n            self.chat_model = chat_model\n            self.transcription_model = transcription_model\n\n        self._forced_provider = provider\n\n    @staticmethod\n    def _default_for_provider(provider: str, capability: str) -&gt; str:\n        \"\"\"Return the default model for a provider/capability combo.\"\"\"\n        defaults = {\n            \"openai\": {\"chat\": \"gpt-4o\", \"vision\": \"gpt-4o\", \"audio\": \"whisper-1\"},\n            \"anthropic\": {\n                \"chat\": \"claude-sonnet-4-5-20250929\",\n                \"vision\": \"claude-sonnet-4-5-20250929\",\n                \"audio\": \"\",\n            },\n            \"gemini\": {\n                \"chat\": \"gemini-2.5-flash\",\n                \"vision\": \"gemini-2.5-flash\",\n                \"audio\": \"gemini-2.5-flash\",\n            },\n            \"ollama\": {\n                \"chat\": \"\",\n                \"vision\": \"\",\n                \"audio\": \"\",\n            },\n        }\n        return defaults.get(provider, {}).get(capability, \"\")\n\n    def _get_provider(self, provider_name: str) -&gt; BaseProvider:\n        \"\"\"Lazily initialize and cache a provider instance.\"\"\"\n        if provider_name not in self._providers:\n            if provider_name == \"openai\":\n                from video_processor.providers.openai_provider import OpenAIProvider\n\n                self._providers[provider_name] = OpenAIProvider()\n            elif provider_name == \"anthropic\":\n                from video_processor.providers.anthropic_provider import AnthropicProvider\n\n                self._providers[provider_name] = AnthropicProvider()\n            elif provider_name == \"gemini\":\n                from video_processor.providers.gemini_provider import GeminiProvider\n\n                self._providers[provider_name] = GeminiProvider()\n            elif provider_name == \"ollama\":\n                from video_processor.providers.ollama_provider import OllamaProvider\n\n                self._providers[provider_name] = OllamaProvider()\n            else:\n                raise ValueError(f\"Unknown provider: {provider_name}\")\n        return self._providers[provider_name]\n\n    def _provider_for_model(self, model_id: str) -&gt; str:\n        \"\"\"Infer the provider from a model id.\"\"\"\n        if (\n            model_id.startswith(\"gpt-\")\n            or model_id.startswith(\"o1\")\n            or model_id.startswith(\"o3\")\n            or model_id.startswith(\"o4\")\n            or model_id.startswith(\"whisper\")\n        ):\n            return \"openai\"\n        if model_id.startswith(\"claude-\"):\n            return \"anthropic\"\n        if model_id.startswith(\"gemini-\"):\n            return \"gemini\"\n        # Try discovery (exact match, then prefix match for ollama name:tag format)\n        models = self._get_available_models()\n        for m in models:\n            if m.id == model_id:\n                return m.provider\n        for m in models:\n            if m.id.startswith(model_id + \":\"):\n                return m.provider\n        raise ValueError(f\"Cannot determine provider for model: {model_id}\")\n\n    def _get_available_models(self) -&gt; list[ModelInfo]:\n        if self._available_models is None:\n            self._available_models = discover_available_models()\n        return self._available_models\n\n    def _resolve_model(\n        self, explicit: Optional[str], capability: str, preferences: list[tuple[str, str]]\n    ) -&gt; tuple[str, str]:\n        \"\"\"\n        Resolve which (provider, model) to use for a capability.\n\n        Returns (provider_name, model_id).\n        \"\"\"\n        if explicit:\n            prov = self._provider_for_model(explicit)\n            return prov, explicit\n\n        if self.auto:\n            # Try preference order, picking the first provider that has an API key\n            for prov, model in preferences:\n                try:\n                    self._get_provider(prov)\n                    return prov, model\n                except (ValueError, ImportError):\n                    continue\n\n            # Fallback: try Ollama if available (no API key needed)\n            try:\n                from video_processor.providers.ollama_provider import OllamaProvider\n\n                if OllamaProvider.is_available():\n                    provider = self._get_provider(\"ollama\")\n                    models = provider.list_models()\n                    for m in models:\n                        if capability in m.capabilities:\n                            return \"ollama\", m.id\n            except Exception:\n                pass\n\n        raise RuntimeError(\n            f\"No provider available for capability '{capability}'. \"\n            \"Set an API key for at least one provider, or start Ollama.\"\n        )\n\n    def _track(self, provider: BaseProvider, prov_name: str, model: str) -&gt; None:\n        \"\"\"Record usage from the last API call on a provider.\"\"\"\n        last = getattr(provider, \"_last_usage\", None)\n        if last:\n            self.usage.record(\n                provider=prov_name,\n                model=model,\n                input_tokens=last.get(\"input_tokens\", 0),\n                output_tokens=last.get(\"output_tokens\", 0),\n            )\n            provider._last_usage = None\n\n    # --- Public API ---\n\n    def chat(\n        self,\n        messages: list[dict],\n        max_tokens: int = 4096,\n        temperature: float = 0.7,\n    ) -&gt; str:\n        \"\"\"Send a chat completion to the best available provider.\"\"\"\n        prov_name, model = self._resolve_model(self.chat_model, \"chat\", _CHAT_PREFERENCES)\n        logger.info(f\"Chat: using {prov_name}/{model}\")\n        provider = self._get_provider(prov_name)\n        result = provider.chat(\n            messages, max_tokens=max_tokens, temperature=temperature, model=model\n        )\n        self._track(provider, prov_name, model)\n        return result\n\n    def analyze_image(\n        self,\n        image_bytes: bytes,\n        prompt: str,\n        max_tokens: int = 4096,\n    ) -&gt; str:\n        \"\"\"Analyze an image using the best available vision provider.\"\"\"\n        prov_name, model = self._resolve_model(self.vision_model, \"vision\", _VISION_PREFERENCES)\n        logger.info(f\"Vision: using {prov_name}/{model}\")\n        provider = self._get_provider(prov_name)\n        result = provider.analyze_image(image_bytes, prompt, max_tokens=max_tokens, model=model)\n        self._track(provider, prov_name, model)\n        return result\n\n    def transcribe_audio(\n        self,\n        audio_path: str | Path,\n        language: Optional[str] = None,\n    ) -&gt; dict:\n        \"\"\"Transcribe audio using local Whisper if available, otherwise API.\"\"\"\n        # Prefer local Whisper \u2014 no file size limits, no API costs\n        if not self.transcription_model or self.transcription_model.startswith(\"whisper-local\"):\n            try:\n                from video_processor.providers.whisper_local import WhisperLocal\n\n                if WhisperLocal.is_available():\n                    # Parse model size from \"whisper-local:large\" or default to \"large\"\n                    size = \"large\"\n                    if self.transcription_model and \":\" in self.transcription_model:\n                        size = self.transcription_model.split(\":\", 1)[1]\n                    if not hasattr(self, \"_whisper_local\"):\n                        self._whisper_local = WhisperLocal(model_size=size)\n                    logger.info(f\"Transcription: using local whisper-{size}\")\n                    result = self._whisper_local.transcribe(audio_path, language=language)\n                    duration = result.get(\"duration\") or 0\n                    self.usage.record(\n                        provider=\"local\",\n                        model=f\"whisper-{size}\",\n                        audio_minutes=duration / 60 if duration else 0,\n                    )\n                    return result\n            except ImportError:\n                pass\n\n        # Fall back to API-based transcription\n        prov_name, model = self._resolve_model(\n            self.transcription_model, \"audio\", _TRANSCRIPTION_PREFERENCES\n        )\n        logger.info(f\"Transcription: using {prov_name}/{model}\")\n        provider = self._get_provider(prov_name)\n        result = provider.transcribe_audio(audio_path, language=language, model=model)\n        duration = result.get(\"duration\") or 0\n        self.usage.record(\n            provider=prov_name,\n            model=model,\n            audio_minutes=duration / 60 if duration else 0,\n        )\n        return result\n\n    def get_models_used(self) -&gt; dict[str, str]:\n        \"\"\"Return a dict mapping capability to 'provider/model' for tracking.\"\"\"\n        result = {}\n        for cap, explicit, prefs in [\n            (\"vision\", self.vision_model, _VISION_PREFERENCES),\n            (\"chat\", self.chat_model, _CHAT_PREFERENCES),\n            (\"transcription\", self.transcription_model, _TRANSCRIPTION_PREFERENCES),\n        ]:\n            try:\n                prov, model = self._resolve_model(explicit, cap, prefs)\n                result[cap] = f\"{prov}/{model}\"\n            except RuntimeError:\n                pass\n        return result\n</code></pre>"},{"location":"api/providers/#video_processor.providers.manager.ProviderManager.__init__","title":"<code>__init__(vision_model=None, chat_model=None, transcription_model=None, provider=None, auto=True)</code>","text":"<p>Initialize the ProviderManager.</p>"},{"location":"api/providers/#video_processor.providers.manager.ProviderManager.__init__--parameters","title":"Parameters","text":"<p>vision_model : override model for vision tasks (e.g. 'gpt-4o') chat_model : override model for chat/LLM tasks transcription_model : override model for transcription provider : force all tasks to a single provider ('openai', 'anthropic', 'gemini') auto : if True and no model specified, pick the best available</p> Source code in <code>video_processor/providers/manager.py</code> <pre><code>def __init__(\n    self,\n    vision_model: Optional[str] = None,\n    chat_model: Optional[str] = None,\n    transcription_model: Optional[str] = None,\n    provider: Optional[str] = None,\n    auto: bool = True,\n):\n    \"\"\"\n    Initialize the ProviderManager.\n\n    Parameters\n    ----------\n    vision_model : override model for vision tasks (e.g. 'gpt-4o')\n    chat_model : override model for chat/LLM tasks\n    transcription_model : override model for transcription\n    provider : force all tasks to a single provider ('openai', 'anthropic', 'gemini')\n    auto : if True and no model specified, pick the best available\n    \"\"\"\n    self.auto = auto\n    self._providers: dict[str, BaseProvider] = {}\n    self._available_models: Optional[list[ModelInfo]] = None\n    self.usage = UsageTracker()\n\n    # If a single provider is forced, apply it\n    if provider:\n        self.vision_model = vision_model or self._default_for_provider(provider, \"vision\")\n        self.chat_model = chat_model or self._default_for_provider(provider, \"chat\")\n        self.transcription_model = transcription_model or self._default_for_provider(\n            provider, \"audio\"\n        )\n    else:\n        self.vision_model = vision_model\n        self.chat_model = chat_model\n        self.transcription_model = transcription_model\n\n    self._forced_provider = provider\n</code></pre>"},{"location":"api/providers/#video_processor.providers.manager.ProviderManager.analyze_image","title":"<code>analyze_image(image_bytes, prompt, max_tokens=4096)</code>","text":"<p>Analyze an image using the best available vision provider.</p> Source code in <code>video_processor/providers/manager.py</code> <pre><code>def analyze_image(\n    self,\n    image_bytes: bytes,\n    prompt: str,\n    max_tokens: int = 4096,\n) -&gt; str:\n    \"\"\"Analyze an image using the best available vision provider.\"\"\"\n    prov_name, model = self._resolve_model(self.vision_model, \"vision\", _VISION_PREFERENCES)\n    logger.info(f\"Vision: using {prov_name}/{model}\")\n    provider = self._get_provider(prov_name)\n    result = provider.analyze_image(image_bytes, prompt, max_tokens=max_tokens, model=model)\n    self._track(provider, prov_name, model)\n    return result\n</code></pre>"},{"location":"api/providers/#video_processor.providers.manager.ProviderManager.chat","title":"<code>chat(messages, max_tokens=4096, temperature=0.7)</code>","text":"<p>Send a chat completion to the best available provider.</p> Source code in <code>video_processor/providers/manager.py</code> <pre><code>def chat(\n    self,\n    messages: list[dict],\n    max_tokens: int = 4096,\n    temperature: float = 0.7,\n) -&gt; str:\n    \"\"\"Send a chat completion to the best available provider.\"\"\"\n    prov_name, model = self._resolve_model(self.chat_model, \"chat\", _CHAT_PREFERENCES)\n    logger.info(f\"Chat: using {prov_name}/{model}\")\n    provider = self._get_provider(prov_name)\n    result = provider.chat(\n        messages, max_tokens=max_tokens, temperature=temperature, model=model\n    )\n    self._track(provider, prov_name, model)\n    return result\n</code></pre>"},{"location":"api/providers/#video_processor.providers.manager.ProviderManager.get_models_used","title":"<code>get_models_used()</code>","text":"<p>Return a dict mapping capability to 'provider/model' for tracking.</p> Source code in <code>video_processor/providers/manager.py</code> <pre><code>def get_models_used(self) -&gt; dict[str, str]:\n    \"\"\"Return a dict mapping capability to 'provider/model' for tracking.\"\"\"\n    result = {}\n    for cap, explicit, prefs in [\n        (\"vision\", self.vision_model, _VISION_PREFERENCES),\n        (\"chat\", self.chat_model, _CHAT_PREFERENCES),\n        (\"transcription\", self.transcription_model, _TRANSCRIPTION_PREFERENCES),\n    ]:\n        try:\n            prov, model = self._resolve_model(explicit, cap, prefs)\n            result[cap] = f\"{prov}/{model}\"\n        except RuntimeError:\n            pass\n    return result\n</code></pre>"},{"location":"api/providers/#video_processor.providers.manager.ProviderManager.transcribe_audio","title":"<code>transcribe_audio(audio_path, language=None)</code>","text":"<p>Transcribe audio using local Whisper if available, otherwise API.</p> Source code in <code>video_processor/providers/manager.py</code> <pre><code>def transcribe_audio(\n    self,\n    audio_path: str | Path,\n    language: Optional[str] = None,\n) -&gt; dict:\n    \"\"\"Transcribe audio using local Whisper if available, otherwise API.\"\"\"\n    # Prefer local Whisper \u2014 no file size limits, no API costs\n    if not self.transcription_model or self.transcription_model.startswith(\"whisper-local\"):\n        try:\n            from video_processor.providers.whisper_local import WhisperLocal\n\n            if WhisperLocal.is_available():\n                # Parse model size from \"whisper-local:large\" or default to \"large\"\n                size = \"large\"\n                if self.transcription_model and \":\" in self.transcription_model:\n                    size = self.transcription_model.split(\":\", 1)[1]\n                if not hasattr(self, \"_whisper_local\"):\n                    self._whisper_local = WhisperLocal(model_size=size)\n                logger.info(f\"Transcription: using local whisper-{size}\")\n                result = self._whisper_local.transcribe(audio_path, language=language)\n                duration = result.get(\"duration\") or 0\n                self.usage.record(\n                    provider=\"local\",\n                    model=f\"whisper-{size}\",\n                    audio_minutes=duration / 60 if duration else 0,\n                )\n                return result\n        except ImportError:\n            pass\n\n    # Fall back to API-based transcription\n    prov_name, model = self._resolve_model(\n        self.transcription_model, \"audio\", _TRANSCRIPTION_PREFERENCES\n    )\n    logger.info(f\"Transcription: using {prov_name}/{model}\")\n    provider = self._get_provider(prov_name)\n    result = provider.transcribe_audio(audio_path, language=language, model=model)\n    duration = result.get(\"duration\") or 0\n    self.usage.record(\n        provider=prov_name,\n        model=model,\n        audio_minutes=duration / 60 if duration else 0,\n    )\n    return result\n</code></pre>"},{"location":"api/providers/#video_processor.providers.discovery","title":"<code>video_processor.providers.discovery</code>","text":"<p>Auto-discover available models across providers.</p>"},{"location":"api/providers/#video_processor.providers.discovery.clear_discovery_cache","title":"<code>clear_discovery_cache()</code>","text":"<p>Clear the cached model list.</p> Source code in <code>video_processor/providers/discovery.py</code> <pre><code>def clear_discovery_cache() -&gt; None:\n    \"\"\"Clear the cached model list.\"\"\"\n    global _cached_models\n    _cached_models = None\n</code></pre>"},{"location":"api/providers/#video_processor.providers.discovery.discover_available_models","title":"<code>discover_available_models(api_keys=None, force_refresh=False)</code>","text":"<p>Discover available models from all configured providers.</p> <p>For each provider with a valid API key, calls list_models() and returns a unified list. Results are cached for the session.</p> Source code in <code>video_processor/providers/discovery.py</code> <pre><code>def discover_available_models(\n    api_keys: Optional[dict[str, str]] = None,\n    force_refresh: bool = False,\n) -&gt; list[ModelInfo]:\n    \"\"\"\n    Discover available models from all configured providers.\n\n    For each provider with a valid API key, calls list_models() and returns\n    a unified list. Results are cached for the session.\n    \"\"\"\n    global _cached_models\n    if _cached_models is not None and not force_refresh:\n        return _cached_models\n\n    keys = api_keys or {\n        \"openai\": os.getenv(\"OPENAI_API_KEY\", \"\"),\n        \"anthropic\": os.getenv(\"ANTHROPIC_API_KEY\", \"\"),\n        \"gemini\": os.getenv(\"GEMINI_API_KEY\", \"\"),\n    }\n\n    all_models: list[ModelInfo] = []\n\n    # OpenAI\n    if keys.get(\"openai\"):\n        try:\n            from video_processor.providers.openai_provider import OpenAIProvider\n\n            provider = OpenAIProvider(api_key=keys[\"openai\"])\n            models = provider.list_models()\n            logger.info(f\"Discovered {len(models)} OpenAI models\")\n            all_models.extend(models)\n        except Exception as e:\n            logger.info(f\"OpenAI discovery skipped: {e}\")\n\n    # Anthropic\n    if keys.get(\"anthropic\"):\n        try:\n            from video_processor.providers.anthropic_provider import AnthropicProvider\n\n            provider = AnthropicProvider(api_key=keys[\"anthropic\"])\n            models = provider.list_models()\n            logger.info(f\"Discovered {len(models)} Anthropic models\")\n            all_models.extend(models)\n        except Exception as e:\n            logger.info(f\"Anthropic discovery skipped: {e}\")\n\n    # Gemini (API key or service account)\n    gemini_key = keys.get(\"gemini\")\n    gemini_creds = os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\", \"\")\n    if gemini_key or gemini_creds:\n        try:\n            from video_processor.providers.gemini_provider import GeminiProvider\n\n            provider = GeminiProvider(\n                api_key=gemini_key or None,\n                credentials_path=gemini_creds or None,\n            )\n            models = provider.list_models()\n            logger.info(f\"Discovered {len(models)} Gemini models\")\n            all_models.extend(models)\n        except Exception as e:\n            logger.warning(f\"Gemini discovery failed: {e}\")\n\n    # Ollama (local, no API key needed)\n    try:\n        from video_processor.providers.ollama_provider import OllamaProvider\n\n        if OllamaProvider.is_available():\n            provider = OllamaProvider()\n            models = provider.list_models()\n            logger.info(f\"Discovered {len(models)} Ollama models\")\n            all_models.extend(models)\n    except Exception as e:\n        logger.info(f\"Ollama discovery skipped: {e}\")\n\n    # Sort by provider then id\n    all_models.sort(key=lambda m: (m.provider, m.id))\n    _cached_models = all_models\n    logger.info(f\"Total discovered models: {len(all_models)}\")\n    return all_models\n</code></pre>"},{"location":"architecture/overview/","title":"Architecture Overview","text":""},{"location":"architecture/overview/#system-diagram","title":"System diagram","text":"<pre><code>graph TD\n    A[Video Input] --&gt; B[Frame Extractor]\n    A --&gt; C[Audio Extractor]\n    B --&gt; D[Diagram Analyzer]\n    C --&gt; E[Transcription]\n    D --&gt; F[Knowledge Graph]\n    E --&gt; F\n    E --&gt; G[Key Point Extractor]\n    E --&gt; H[Action Item Detector]\n    D --&gt; I[Content Analyzer]\n    E --&gt; I\n    F --&gt; J[Plan Generator]\n    G --&gt; J\n    H --&gt; J\n    I --&gt; J\n    J --&gt; K[Markdown Report]\n    J --&gt; L[HTML Report]\n    J --&gt; M[PDF Report]\n    D --&gt; N[Mermaid/SVG/PNG Export]</code></pre>"},{"location":"architecture/overview/#module-structure","title":"Module structure","text":"<pre><code>video_processor/\n\u251c\u2500\u2500 cli/                    # CLI commands (Click)\n\u2502   \u2514\u2500\u2500 commands.py\n\u251c\u2500\u2500 extractors/             # Media extraction\n\u2502   \u251c\u2500\u2500 frame_extractor.py  # Video \u2192 frames\n\u2502   \u2514\u2500\u2500 audio_extractor.py  # Video \u2192 WAV\n\u251c\u2500\u2500 analyzers/              # AI-powered analysis\n\u2502   \u251c\u2500\u2500 diagram_analyzer.py # Frame classification + extraction\n\u2502   \u251c\u2500\u2500 content_analyzer.py # Cross-referencing\n\u2502   \u2514\u2500\u2500 action_detector.py  # Action item detection\n\u251c\u2500\u2500 integrators/            # Knowledge assembly\n\u2502   \u251c\u2500\u2500 knowledge_graph.py  # Entity/relationship graph\n\u2502   \u2514\u2500\u2500 plan_generator.py   # Report generation\n\u251c\u2500\u2500 providers/              # AI provider abstraction\n\u2502   \u251c\u2500\u2500 base.py             # BaseProvider ABC\n\u2502   \u251c\u2500\u2500 openai_provider.py\n\u2502   \u251c\u2500\u2500 anthropic_provider.py\n\u2502   \u251c\u2500\u2500 gemini_provider.py\n\u2502   \u251c\u2500\u2500 ollama_provider.py  # Local Ollama (offline)\n\u2502   \u251c\u2500\u2500 discovery.py        # Auto-model-discovery\n\u2502   \u2514\u2500\u2500 manager.py          # ProviderManager routing\n\u251c\u2500\u2500 utils/\n\u2502   \u251c\u2500\u2500 json_parsing.py     # Robust LLM JSON parsing\n\u2502   \u251c\u2500\u2500 rendering.py        # Mermaid + chart rendering\n\u2502   \u251c\u2500\u2500 export.py           # HTML/PDF export\n\u2502   \u251c\u2500\u2500 api_cache.py        # Disk-based response cache\n\u2502   \u2514\u2500\u2500 prompt_templates.py # LLM prompt management\n\u251c\u2500\u2500 models.py               # Pydantic data models\n\u251c\u2500\u2500 output_structure.py     # Directory layout + manifest I/O\n\u2514\u2500\u2500 pipeline.py             # Core processing pipeline\n</code></pre>"},{"location":"architecture/overview/#key-design-decisions","title":"Key design decisions","text":"<ul> <li>Pydantic everywhere \u2014 All structured data uses pydantic models for validation and serialization</li> <li>Manifest-driven \u2014 Every run produces <code>manifest.json</code> as the single source of truth</li> <li>Provider abstraction \u2014 Single <code>ProviderManager</code> wraps OpenAI, Anthropic, Gemini, and Ollama behind a common interface</li> <li>No hardcoded models \u2014 Model lists come from API discovery</li> <li>Screengrab fallback \u2014 When extraction fails, save the frame as a captioned screenshot</li> </ul>"},{"location":"architecture/pipeline/","title":"Processing Pipeline","text":""},{"location":"architecture/pipeline/#single-video-pipeline","title":"Single video pipeline","text":"<pre><code>sequenceDiagram\n    participant CLI\n    participant Pipeline\n    participant FrameExtractor\n    participant AudioExtractor\n    participant Provider\n    participant DiagramAnalyzer\n    participant KnowledgeGraph\n\n    CLI-&gt;&gt;Pipeline: process_single_video()\n    Pipeline-&gt;&gt;FrameExtractor: extract_frames()\n    Note over FrameExtractor: Change detection + periodic capture (every 30s)\n    Pipeline-&gt;&gt;Pipeline: filter_people_frames()\n    Note over Pipeline: OpenCV face detection removes webcam/people frames\n    Pipeline-&gt;&gt;AudioExtractor: extract_audio()\n    Pipeline-&gt;&gt;Provider: transcribe_audio()\n    Pipeline-&gt;&gt;DiagramAnalyzer: process_frames()\n\n    loop Each frame\n        DiagramAnalyzer-&gt;&gt;Provider: classify (vision)\n        alt High confidence diagram\n            DiagramAnalyzer-&gt;&gt;Provider: full analysis\n        else Medium confidence\n            DiagramAnalyzer--&gt;&gt;Pipeline: screengrab fallback\n        end\n    end\n\n    Pipeline-&gt;&gt;KnowledgeGraph: process_transcript()\n    Pipeline-&gt;&gt;KnowledgeGraph: process_diagrams()\n    Pipeline-&gt;&gt;Provider: extract key points\n    Pipeline-&gt;&gt;Provider: extract action items\n    Pipeline-&gt;&gt;Pipeline: generate reports\n    Pipeline-&gt;&gt;Pipeline: export formats\n    Pipeline--&gt;&gt;CLI: VideoManifest</code></pre>"},{"location":"architecture/pipeline/#batch-pipeline","title":"Batch pipeline","text":"<p>The batch command wraps the single-video pipeline:</p> <ol> <li>Scan input directory for matching video files</li> <li>For each video: <code>process_single_video()</code> with error handling</li> <li>Merge knowledge graphs across all completed videos</li> <li>Generate batch summary with aggregated stats</li> <li>Write batch manifest</li> </ol>"},{"location":"architecture/pipeline/#error-handling","title":"Error handling","text":"<ul> <li>Individual video failures don't stop the batch</li> <li>Failed videos are logged with error details in the manifest</li> <li>Diagram analysis failures fall back to screengrabs</li> <li>LLM extraction failures return empty results gracefully</li> </ul>"},{"location":"architecture/providers/","title":"Provider System","text":""},{"location":"architecture/providers/#overview","title":"Overview","text":"<p>PlanOpticon supports multiple AI providers through a unified abstraction layer.</p>"},{"location":"architecture/providers/#supported-providers","title":"Supported providers","text":"Provider Chat Vision Transcription OpenAI GPT-4o, GPT-4 GPT-4o Whisper-1 Anthropic Claude Sonnet/Opus Claude Sonnet/Opus \u2014 Google Gemini Gemini Flash/Pro Gemini Flash/Pro Gemini Flash Ollama (local) Any installed model llava, moondream, etc. \u2014 (use local Whisper)"},{"location":"architecture/providers/#ollama-offline-mode","title":"Ollama (offline mode)","text":"<p>Ollama enables fully offline operation with no API keys required. PlanOpticon connects via Ollama's OpenAI-compatible API.</p> <pre><code># Install and start Ollama\nollama serve\n\n# Pull a chat model\nollama pull llama3.2\n\n# Pull a vision model (for diagram analysis)\nollama pull llava\n</code></pre> <p>PlanOpticon auto-detects Ollama when it's running. To force Ollama:</p> <pre><code>planopticon analyze -i video.mp4 -o ./out --provider ollama\n</code></pre> <p>Configure a non-default host via <code>OLLAMA_HOST</code>:</p> <pre><code>export OLLAMA_HOST=http://192.168.1.100:11434\n</code></pre>"},{"location":"architecture/providers/#auto-discovery","title":"Auto-discovery","text":"<p>On startup, <code>ProviderManager</code> checks which API keys are configured, queries each provider's API, and checks for a running Ollama server to discover available models:</p> <pre><code>from video_processor.providers.manager import ProviderManager\n\npm = ProviderManager()\n# Automatically discovers models from all configured providers + Ollama\n</code></pre>"},{"location":"architecture/providers/#routing-preferences","title":"Routing preferences","text":"<p>Each task type has a default preference order:</p> Task Preference Vision Gemini Flash \u2192 GPT-4o \u2192 Claude Sonnet \u2192 Ollama Chat Claude Sonnet \u2192 GPT-4o \u2192 Gemini Flash \u2192 Ollama Transcription Local Whisper \u2192 Whisper-1 \u2192 Gemini Flash <p>Ollama acts as the last-resort fallback \u2014 if no cloud API keys are set but Ollama is running, it is used automatically.</p>"},{"location":"architecture/providers/#manual-override","title":"Manual override","text":"<pre><code>pm = ProviderManager(\n    vision_model=\"gpt-4o\",\n    chat_model=\"claude-sonnet-4-5-20250929\",\n    provider=\"openai\",  # Force a specific provider\n)\n\n# Or use Ollama for fully offline processing\npm = ProviderManager(provider=\"ollama\")\n</code></pre>"},{"location":"architecture/providers/#baseprovider-interface","title":"BaseProvider interface","text":"<p>All providers implement:</p> <pre><code>class BaseProvider(ABC):\n    def chat(messages, max_tokens, temperature) -&gt; str\n    def analyze_image(image_path, prompt, max_tokens) -&gt; str\n    def transcribe_audio(audio_path) -&gt; dict\n    def list_models() -&gt; List[ModelInfo]\n</code></pre>"},{"location":"getting-started/configuration/","title":"Configuration","text":""},{"location":"getting-started/configuration/#environment-variables","title":"Environment variables","text":"Variable Description <code>OPENAI_API_KEY</code> OpenAI API key <code>ANTHROPIC_API_KEY</code> Anthropic API key <code>GEMINI_API_KEY</code> Google Gemini API key <code>OLLAMA_HOST</code> Ollama server URL (default: <code>http://localhost:11434</code>) <code>GOOGLE_APPLICATION_CREDENTIALS</code> Path to Google service account JSON (for Drive) <code>CACHE_DIR</code> Directory for API response caching"},{"location":"getting-started/configuration/#provider-routing","title":"Provider routing","text":"<p>PlanOpticon auto-discovers available models and routes each task to the best option:</p> Task Default preference Vision (diagrams) Gemini Flash &gt; GPT-4o &gt; Claude Sonnet &gt; Ollama Chat (analysis) Claude Sonnet &gt; GPT-4o &gt; Gemini Flash &gt; Ollama Transcription Local Whisper &gt; Whisper-1 &gt; Gemini Flash <p>If no cloud API keys are configured, PlanOpticon automatically falls back to Ollama when a local server is running. This enables fully offline operation when paired with local Whisper for transcription.</p> <p>Override with <code>--provider</code>, <code>--vision-model</code>, or <code>--chat-model</code> flags.</p>"},{"location":"getting-started/configuration/#frame-sampling","title":"Frame sampling","text":"<p>Control how frames are extracted:</p> <pre><code># Sample rate: frames per second (default: 0.5)\nplanopticon analyze -i video.mp4 -o ./out --sampling-rate 1.0\n\n# Change threshold: visual difference needed to keep a frame (default: 0.15)\nplanopticon analyze -i video.mp4 -o ./out --change-threshold 0.1\n\n# Periodic capture: capture a frame every N seconds regardless of change (default: 30)\n# Useful for slow-evolving content like document scrolling\nplanopticon analyze -i video.mp4 -o ./out --periodic-capture 15\n\n# Disable periodic capture (rely only on change detection)\nplanopticon analyze -i video.mp4 -o ./out --periodic-capture 0\n</code></pre> <p>Lower <code>change-threshold</code> = more frames kept. Higher <code>sampling-rate</code> = more candidates. Periodic capture catches content that changes too slowly for change detection (e.g., scrolling through a document during a screen share).</p> <p>People/webcam frames are automatically filtered out using face detection \u2014 no configuration needed.</p>"},{"location":"getting-started/configuration/#focus-areas","title":"Focus areas","text":"<p>Limit processing to specific extraction types:</p> <pre><code>planopticon analyze -i video.mp4 -o ./out --focus \"diagrams,action-items\"\n</code></pre>"},{"location":"getting-started/configuration/#gpu-acceleration","title":"GPU acceleration","text":"<pre><code>planopticon analyze -i video.mp4 -o ./out --use-gpu\n</code></pre> <p>Requires <code>planopticon[gpu]</code> extras installed.</p>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#from-pypi","title":"From PyPI","text":"<pre><code>pip install planopticon\n</code></pre>"},{"location":"getting-started/installation/#optional-extras","title":"Optional extras","text":"<pre><code># PDF export support\npip install planopticon[pdf]\n\n# Google Drive + Dropbox integration\npip install planopticon[cloud]\n\n# GPU acceleration\npip install planopticon[gpu]\n\n# Everything\npip install planopticon[all]\n</code></pre>"},{"location":"getting-started/installation/#from-source","title":"From source","text":"<pre><code>git clone https://github.com/ConflictHQ/PlanOpticon.git\ncd PlanOpticon\npip install -e \".[dev]\"\n</code></pre>"},{"location":"getting-started/installation/#binary-download","title":"Binary download","text":"<p>Download standalone binaries (no Python required) from GitHub Releases:</p> Platform Download macOS (Apple Silicon) <code>planopticon-macos-arm64</code> macOS (Intel) <code>planopticon-macos-x86_64</code> Linux (x86_64) <code>planopticon-linux-x86_64</code> Windows <code>planopticon-windows-x86_64.exe</code>"},{"location":"getting-started/installation/#system-dependencies","title":"System dependencies","text":"<p>PlanOpticon requires FFmpeg for audio extraction:</p> macOSUbuntu/DebianWindows <pre><code>brew install ffmpeg\n</code></pre> <pre><code>sudo apt-get install ffmpeg libsndfile1\n</code></pre> <p>Download from ffmpeg.org and add to PATH.</p>"},{"location":"getting-started/installation/#api-keys","title":"API keys","text":"<p>You need at least one AI provider API key or a running Ollama server.</p>"},{"location":"getting-started/installation/#cloud-providers","title":"Cloud providers","text":"<p>Set API keys as environment variables:</p> <pre><code>export OPENAI_API_KEY=\"sk-...\"\nexport ANTHROPIC_API_KEY=\"sk-ant-...\"\nexport GEMINI_API_KEY=\"AI...\"\n</code></pre> <p>Or create a <code>.env</code> file in your project directory:</p> <pre><code>OPENAI_API_KEY=sk-...\nANTHROPIC_API_KEY=sk-ant-...\nGEMINI_API_KEY=AI...\n</code></pre>"},{"location":"getting-started/installation/#ollama-fully-offline","title":"Ollama (fully offline)","text":"<p>No API keys needed \u2014 just install and run Ollama:</p> <pre><code># Install Ollama, then pull models\nollama pull llama3.2        # Chat/analysis\nollama pull llava            # Vision (diagram detection)\n\n# Start the server (if not already running)\nollama serve\n</code></pre> <p>PlanOpticon auto-detects Ollama and uses it as a fallback when no cloud API keys are set. For a fully offline pipeline, pair Ollama with local Whisper transcription (<code>pip install planopticon[gpu]</code>).</p> <p>PlanOpticon will automatically discover which providers are available and route to the best model for each task.</p>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":""},{"location":"getting-started/quickstart/#analyze-a-single-video","title":"Analyze a single video","text":"<pre><code>planopticon analyze -i meeting.mp4 -o ./output\n</code></pre> <p>This runs the full pipeline:</p> <ol> <li>Extracts video frames (smart sampling, change detection)</li> <li>Extracts and transcribes audio</li> <li>Detects and analyzes diagrams, charts, whiteboards</li> <li>Builds a knowledge graph of entities and relationships</li> <li>Extracts key points and action items</li> <li>Generates markdown, HTML, and PDF reports</li> <li>Outputs a <code>manifest.json</code> with everything</li> </ol>"},{"location":"getting-started/quickstart/#processing-depth","title":"Processing depth","text":"<pre><code># Quick scan \u2014 transcription + key points only\nplanopticon analyze -i video.mp4 -o ./out --depth basic\n\n# Standard \u2014 includes diagram extraction (default)\nplanopticon analyze -i video.mp4 -o ./out --depth standard\n\n# Deep \u2014 more frames analyzed, richer extraction\nplanopticon analyze -i video.mp4 -o ./out --depth comprehensive\n</code></pre>"},{"location":"getting-started/quickstart/#choose-a-provider","title":"Choose a provider","text":"<pre><code># Auto-detect best available (default)\nplanopticon analyze -i video.mp4 -o ./out\n\n# Force a specific provider\nplanopticon analyze -i video.mp4 -o ./out --provider openai\n\n# Use Ollama for fully offline processing (no API keys needed)\nplanopticon analyze -i video.mp4 -o ./out --provider ollama\n\n# Override specific models\nplanopticon analyze -i video.mp4 -o ./out \\\n    --vision-model gpt-4o \\\n    --chat-model claude-sonnet-4-5-20250929\n</code></pre>"},{"location":"getting-started/quickstart/#batch-processing","title":"Batch processing","text":"<pre><code># Process all videos in a folder\nplanopticon batch -i ./recordings -o ./output\n\n# Custom file patterns\nplanopticon batch -i ./recordings -o ./output --pattern \"*.mp4,*.mov\"\n\n# With a title for the batch report\nplanopticon batch -i ./recordings -o ./output --title \"Q4 Sprint Reviews\"\n</code></pre> <p>Batch mode produces per-video outputs plus:</p> <ul> <li>Merged knowledge graph across all videos</li> <li>Batch summary with aggregated action items</li> <li>Cross-referenced entities</li> </ul>"},{"location":"getting-started/quickstart/#discover-available-models","title":"Discover available models","text":"<pre><code>planopticon list-models\n</code></pre> <p>Shows all models from configured providers with their capabilities (vision, chat, transcription).</p>"},{"location":"getting-started/quickstart/#output-structure","title":"Output structure","text":"<p>After processing, your output directory looks like:</p> <pre><code>output/\n\u251c\u2500\u2500 manifest.json          # Single source of truth\n\u251c\u2500\u2500 transcript/\n\u2502   \u251c\u2500\u2500 transcript.json    # Full transcript with segments\n\u2502   \u251c\u2500\u2500 transcript.txt     # Plain text\n\u2502   \u2514\u2500\u2500 transcript.srt     # Subtitles\n\u251c\u2500\u2500 frames/                # Extracted video frames\n\u251c\u2500\u2500 diagrams/              # Detected diagrams + mermaid/SVG/PNG\n\u251c\u2500\u2500 captures/              # Screengrab fallbacks\n\u2514\u2500\u2500 results/\n    \u251c\u2500\u2500 analysis.md        # Markdown report\n    \u251c\u2500\u2500 analysis.html      # HTML report\n    \u251c\u2500\u2500 analysis.pdf       # PDF report\n    \u251c\u2500\u2500 knowledge_graph.json\n    \u251c\u2500\u2500 key_points.json\n    \u2514\u2500\u2500 action_items.json\n</code></pre>"},{"location":"guide/batch/","title":"Batch Processing","text":""},{"location":"guide/batch/#basic-usage","title":"Basic usage","text":"<pre><code>planopticon batch -i ./recordings -o ./output --title \"Sprint Reviews\"\n</code></pre>"},{"location":"guide/batch/#how-it-works","title":"How it works","text":"<p>Batch mode:</p> <ol> <li>Scans the input directory for video files matching the pattern</li> <li>Processes each video through the full single-video pipeline</li> <li>Merges knowledge graphs across all videos (case-insensitive entity dedup)</li> <li>Generates a batch summary with aggregated stats and action items</li> <li>Writes a batch manifest linking to per-video results</li> </ol>"},{"location":"guide/batch/#file-patterns","title":"File patterns","text":"<pre><code># Default: common video formats\nplanopticon batch -i ./recordings -o ./output\n\n# Custom patterns\nplanopticon batch -i ./recordings -o ./output --pattern \"*.mp4,*.mov\"\n</code></pre>"},{"location":"guide/batch/#output-structure","title":"Output structure","text":"<pre><code>output/\n\u251c\u2500\u2500 batch_manifest.json       # Batch-level manifest\n\u251c\u2500\u2500 batch_summary.md          # Aggregated summary\n\u251c\u2500\u2500 knowledge_graph.json      # Merged KG across all videos\n\u2514\u2500\u2500 videos/\n    \u251c\u2500\u2500 meeting-01/\n    \u2502   \u251c\u2500\u2500 manifest.json\n    \u2502   \u251c\u2500\u2500 transcript/\n    \u2502   \u251c\u2500\u2500 diagrams/\n    \u2502   \u2514\u2500\u2500 results/\n    \u2514\u2500\u2500 meeting-02/\n        \u251c\u2500\u2500 manifest.json\n        \u2514\u2500\u2500 ...\n</code></pre>"},{"location":"guide/batch/#knowledge-graph-merging","title":"Knowledge graph merging","text":"<p>When the same entity appears across multiple videos, PlanOpticon merges them:</p> <ul> <li>Case-insensitive name matching</li> <li>Descriptions are unioned</li> <li>Occurrences are concatenated with source tracking</li> <li>Relationships are deduplicated</li> </ul> <p>The merged knowledge graph is saved at the batch root and included in the batch summary as a mermaid diagram.</p>"},{"location":"guide/batch/#error-handling","title":"Error handling","text":"<p>If a video fails to process, the batch continues. Failed videos are recorded in the batch manifest with error details:</p> <pre><code>{\n  \"video_name\": \"corrupted-file\",\n  \"status\": \"failed\",\n  \"error\": \"Audio extraction failed: no audio track found\"\n}\n</code></pre>"},{"location":"guide/cloud-sources/","title":"Cloud Sources","text":"<p>PlanOpticon can fetch videos directly from cloud storage services.</p>"},{"location":"guide/cloud-sources/#google-drive","title":"Google Drive","text":""},{"location":"guide/cloud-sources/#service-account-auth","title":"Service account auth","text":"<p>For automated/server-side usage:</p> <pre><code>export GOOGLE_APPLICATION_CREDENTIALS=\"/path/to/service-account.json\"\nplanopticon batch --source gdrive --folder-id \"abc123\" -o ./output\n</code></pre>"},{"location":"guide/cloud-sources/#oauth2-user-auth","title":"OAuth2 user auth","text":"<p>For interactive usage with your own Google account:</p> <pre><code>planopticon auth google\nplanopticon batch --source gdrive --folder-id \"abc123\" -o ./output\n</code></pre>"},{"location":"guide/cloud-sources/#install","title":"Install","text":"<pre><code>pip install planopticon[gdrive]\n</code></pre>"},{"location":"guide/cloud-sources/#dropbox","title":"Dropbox","text":""},{"location":"guide/cloud-sources/#oauth2-auth","title":"OAuth2 auth","text":"<pre><code>planopticon auth dropbox\nplanopticon batch --source dropbox --folder \"/Recordings\" -o ./output\n</code></pre>"},{"location":"guide/cloud-sources/#install_1","title":"Install","text":"<pre><code>pip install planopticon[dropbox]\n</code></pre>"},{"location":"guide/cloud-sources/#all-cloud-sources","title":"All cloud sources","text":"<pre><code>pip install planopticon[cloud]\n</code></pre>"},{"location":"guide/output-formats/","title":"Output Formats","text":"<p>PlanOpticon produces multiple output formats from each analysis run.</p>"},{"location":"guide/output-formats/#transcripts","title":"Transcripts","text":"Format File Description JSON <code>transcript/transcript.json</code> Full transcript with segments, timestamps, speakers Text <code>transcript/transcript.txt</code> Plain text transcript SRT <code>transcript/transcript.srt</code> Subtitle format with timestamps"},{"location":"guide/output-formats/#reports","title":"Reports","text":"Format File Description Markdown <code>results/analysis.md</code> Structured report with diagrams HTML <code>results/analysis.html</code> Self-contained HTML with mermaid.js PDF <code>results/analysis.pdf</code> Print-ready PDF (requires <code>planopticon[pdf]</code>)"},{"location":"guide/output-formats/#diagrams","title":"Diagrams","text":"<p>Each detected diagram produces:</p> Format File Description JPEG <code>diagrams/diagram_N.jpg</code> Original frame Mermaid <code>diagrams/diagram_N.mermaid</code> Mermaid source code SVG <code>diagrams/diagram_N.svg</code> Vector rendering PNG <code>diagrams/diagram_N.png</code> Raster rendering JSON <code>diagrams/diagram_N.json</code> Structured analysis data"},{"location":"guide/output-formats/#structured-data","title":"Structured data","text":"Format File Description JSON <code>results/knowledge_graph.json</code> Entities and relationships JSON <code>results/key_points.json</code> Extracted key points JSON <code>results/action_items.json</code> Action items with assignees JSON <code>manifest.json</code> Complete run manifest"},{"location":"guide/output-formats/#charts","title":"Charts","text":"<p>When chart data is extracted from diagrams (bar, line, pie, scatter), PlanOpticon reproduces them:</p> <ul> <li>SVG + PNG via matplotlib</li> <li>Embedded in HTML/PDF reports</li> </ul>"},{"location":"guide/single-video/","title":"Single Video Analysis","text":""},{"location":"guide/single-video/#basic-usage","title":"Basic usage","text":"<pre><code>planopticon analyze -i recording.mp4 -o ./output\n</code></pre>"},{"location":"guide/single-video/#what-happens","title":"What happens","text":"<p>The pipeline runs these steps in order:</p> <ol> <li>Frame extraction \u2014 Samples frames using change detection for transitions plus periodic capture (every 30s) for slow-evolving content like document scrolling</li> <li>People frame filtering \u2014 OpenCV face detection automatically removes webcam/video conference frames, keeping only shared content (slides, documents, screen shares)</li> <li>Audio extraction \u2014 Extracts audio track to WAV</li> <li>Transcription \u2014 Sends audio to speech-to-text (Whisper or Gemini)</li> <li>Diagram detection \u2014 Vision model classifies each frame as diagram/chart/whiteboard/screenshot/none</li> <li>Diagram analysis \u2014 High-confidence diagrams get full extraction (description, text, mermaid, chart data)</li> <li>Screengrab fallback \u2014 Medium-confidence frames are saved as captioned screenshots</li> <li>Knowledge graph \u2014 Extracts entities and relationships from transcript + diagrams</li> <li>Key points \u2014 LLM extracts main points and topics</li> <li>Action items \u2014 LLM finds tasks, commitments, and follow-ups</li> <li>Reports \u2014 Generates markdown, HTML, and PDF</li> <li>Export \u2014 Renders mermaid diagrams to SVG/PNG, reproduces charts</li> </ol>"},{"location":"guide/single-video/#processing-depth","title":"Processing depth","text":""},{"location":"guide/single-video/#basic","title":"<code>basic</code>","text":"<ul> <li>Transcription only</li> <li>Key points and action items</li> <li>No diagram extraction</li> </ul>"},{"location":"guide/single-video/#standard-default","title":"<code>standard</code> (default)","text":"<ul> <li>Everything in basic</li> <li>Diagram extraction (up to 10 frames)</li> <li>Knowledge graph</li> <li>Full report generation</li> </ul>"},{"location":"guide/single-video/#comprehensive","title":"<code>comprehensive</code>","text":"<ul> <li>Everything in standard</li> <li>More frames analyzed (up to 20)</li> <li>Deeper analysis</li> </ul>"},{"location":"guide/single-video/#output-manifest","title":"Output manifest","text":"<p>Every run produces a <code>manifest.json</code> that is the single source of truth:</p> <pre><code>{\n  \"version\": \"1.0\",\n  \"video\": {\n    \"title\": \"Analysis of recording\",\n    \"source_path\": \"/path/to/recording.mp4\",\n    \"duration_seconds\": 3600.0\n  },\n  \"stats\": {\n    \"duration_seconds\": 45.2,\n    \"frames_extracted\": 42,\n    \"people_frames_filtered\": 11,\n    \"diagrams_detected\": 3,\n    \"screen_captures\": 5\n  },\n  \"key_points\": [...],\n  \"action_items\": [...],\n  \"diagrams\": [...],\n  \"screen_captures\": [...]\n}\n</code></pre>"}]}